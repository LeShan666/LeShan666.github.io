<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2019%2F06%2F18%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[排序算法 输入：一个n个数的序列 a1，a2……an输出输入序列的一个排列a1‘，a2‘……n‘，使得a1‘&lt;=a2‘&lt;=……&lt;=an‘ &emsp;&emsp;简而言之，排序就是将一组对象按照某一种逻辑顺序重新排列的过程，这是一个很古老的算法，同时在各个领域都起到了非常重要的作用，所以在这里对排序算法做一次总结。 排序算法与数据结构&emsp;&emsp;在我们进行练习的时候，通常都是使用一个个的数值进行排序，实际上，在应用中，我们不可能只对一个数值进行排序，而是对一个记录进行排序，排序的依据可能是其中的某一个关键字，除关键字之外，记录还会有卫星数据，所以在排序的过程中，并不是只对那些关键字进行排序，卫星数据也是要一同进行存取的。所以，如果卫星数据过大，在排序过程中挪动数据是比较耗费资源的，这种情况下，重新排列记录的指针数组而不是记录本身是一个更好的选择。&emsp;&emsp;正是这些细节上的选择，区分开了程序与算法之间的不同。算法只会描述一个方法，记录之中的卫星数据大小是不考虑的。所以我们在研究排序问题的时候，假定元素由数字组成，这样在概念上比较直接。但是具体情况之中，一些细节问题可能需要我们使用不同的数据结构来解决，这也是一种挑战。 为何使用排序？&emsp;&emsp;上面也说了，排序在各个领域都起到了非常重要的作用。而且这是一个在计算机领域中非常基础的问题。虽然排序算法在许多语言中都被内置来(甚至连C语言也内置了排序算法)，但是对于一个学习者的角度来讲，排序算法往往能够帮助我们有效的解决其他类似的问题。在跟多领域中，排序往往是第一个需要被解决的问题，而且排序算法多种多样，他们实现优雅高效，具有很深的研究价值。下面我会对我学过对一些排序算法进行总结，尽管插入排序与归并排序在算法分析那里已经总结过了，这里作为一个整体性对总结，我会再次总结。 冒泡排序&emsp;&emsp;冒泡排序是最早c语言程序设计课上学习的一个排序，也是大部分人所接触到的第一个排序，因为非常简单。但是因为它的性能很差，所以很多算法书籍中只会一笔带过，甚至就是作为一个课后习题出现。&emsp;&emsp;冒泡排序是交换排序一种，它的原理是比较相邻两个元素，第一个比第二个大，就进行交换，往后每一对相邻的元素都进行这样的操作，直到最后一个，最后一个就是最大的数，持续这个步骤，直到无法继续交换为止，正如同它的名字一样，最大的值是依次以冒气泡的的方式往上冒出来的。看一下具体的排序过程，以整型数组{84,83,88,87,61}举例(我用ppt画个图)。&emsp;&emsp;首先看第一趟冒泡.&emsp;&emsp;第一趟结束以后，可以看见最大值88已经被移动到了最后，接着第二趟.&emsp;&emsp;第二趟之后，第二大的数字87被移动到了倒数第二个。在看第三趟。&emsp;&emsp;第三趟之后，第三大的数字84被移动到了倒数第三个。&emsp;&emsp;经历了四趟之后，整个数组排序完毕。&emsp;&emsp;看一下冒泡排序的复杂度，最好情况下，即待排数据本身就是正序的，只需要一趟即可完成排序，比较次数为n-1，元素移动次数为0(不需要进行交换)，此时拥有最好的时间复杂度，即O(n);&emsp;&emsp;再看一下最坏的情况，最坏情况即是待排数据是逆序的，此时就需要进行n-1趟排序操作，每次要比较n-i(1 ≤ i ≤ N - 1)次,由于完全逆序，每次比较都需要进行交换，而且每次交换都要移动三次数据(两个数据交换需要三次移动)，那么我们的比较次数为 $\frac{N(N-1)}{2}$=O(N2)，移动次数为 $\frac{3N(N-1)}{2}$=O(N2)，他的最坏时间复杂度是O(N2)，因此，它的平均时间复杂度也是O(N2)。由于其中只有在交换过程中会用到一个单位辅助空间，所以其空间复杂度为O(1)。&emsp;&emsp;冒泡排序是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以相同元素的前后顺序并没有改变，冒泡排序是一种稳定排序算法。下面贴上冒泡排序的C++实现。12345678910111213/*冒泡排序*/void bubbsort(int* a, int length) &#123; int temp; for (int i = 0; i &lt; length; i++)&#123; for (int j = length-1; j &gt; i; j--)&#123; if (a[j]&lt;a[j-1])&#123; temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125;&#125; 选择排序&emsp;&emsp;这也是一个非常简单的排序算法，它的排序规则是这样的，首先找到数组中最小的元素，将它与数组中第一个元素的位置交换，然后再从剩下的元素中选取最小的元素，与数组的第二的元素交换。如此往复，直到整个数组有序，因为其不断在待排序元素中选择最大(小)者，所以起名叫选择排序。大致看一下排序过程，以数组{12，2，16，30，28，10，16，20，6，18}为例。&emsp;&emsp;这个排序很简单，每一趟排序只是在比较当前元素与目前已知的最小元素，每一次排序都会将一个元素放在最终位置，从上图也可以看出来，对角线上的每一个元素都代表一次交换，总交换次数为N，0～N-1的任意第i次交换都会伴随着一次交换与N-1-i次比较，所以总共会有N次交换与(N-1)+(N-1)…2+1=N(N-1)/2~N2/2次比较。&emsp;&emsp;在上面对冒泡排序的分析中，提到了最好运行时间与最坏运行时间，但是选择排序有一个特点，它的运行时间与输入无关，每一遍扫描并不能为下一遍提供什么信息或者遍历，所以无论原始序列有序无序、正序逆序，他们用的时间都一样长！它并不会向其他算法一样依赖初始状态，这有时候是个好事，有时候也是个坏事，它的时间复杂度是O(N2)。&emsp;&emsp;选择排序对数据移动可以说是最少的，每次交换都会改变两个数组元素，一共N次交换，并且交换次数与数组长度呈线性关系，这是一个其他算法都不具备的特征(大部分都是线性对数或者平方关系)，虽然选择排序和冒泡排序的时间复杂度在数量级上是一样的，都是两层循环。但是冒泡排序平均的换位操作比选择排序要多，所以内层循环中，选择排序只有O(N)而冒泡排序有O(N2/2)，所以综合来讲选择排序的时间效率要高于冒泡排序。下面贴出代码。1234567891011121314151617void exch(int* a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125;/*选择排序*/void selectsort(int* a, int length) &#123; for (int i = 0; i &lt; length; i++)&#123;//将a[i]与a[i+1...length]的最小元素交换 int min = i;//最小元素索引 for (int j = i+1; j &lt; length; j++)&#123; if (a[j] &lt; a[min]) &#123; min = j; &#125; &#125; exch(a, i, min); &#125;&#125; 插入排序&emsp;&emsp;这个排序在算法分析中作为例子讲到过，现在再来说一遍。这个排序有点像在打牌的时候整理自己的手牌，我们会将未排序的牌插入到已经排好序的手牌之中，而且为了填补空缺，在牌移动前和移动后位置中间的所有牌会向后移动一位，插入排序算法的思路也是如此。&emsp;&emsp;与选择排序一样，插入排序的索引左侧的元素的排列是有序的，但是在排序结束之前，那些元素的位置并不确定，中间随时可能移动，当索引达到另外一侧时，排序完毕。看一下插入排序的过程。&emsp;&emsp;在这个N*N的轨迹表上很容易看出来交换和比较次数，最坏情况下是对角线一下所有元素都要移动位置(当前插入元素比所有一排序元素都要小),最好情况下根本不需要移动(当前插入元素比所有待排元素都大)，插入排序与选择排序不同，它的排序时间取决于初始元素顺序，比如一个有序数组的排序要比逆序或者乱序数组要快很多。&emsp;&emsp;最坏情况下在对角线以下所有元素都要移动与被比较，所以总共N2/2次比较与N2/2移动，而平均情况下，每次排序需要移动与被比较的元素为对角线一下元素总数的二分之一，那么总共N2/4次比较与N2/4移动。当然，最好情况下，它只需要N次比较与0次交换。总体来说，它的时间复杂度也是O(N2)。&emsp;&emsp;这种排序对于某种非随机数组特别有效，比如对一个有序数组来说，插入排序在发现其是有序之后，它的运行时间就会变成线性的。下面贴出实现。1234567891011121314/*插入排序*/void insertSort(int* a, int length) &#123; int key, i; for (int j = 1; j &lt; length; j++) &#123;//从第二个开始遍历，直到最后一个 key = a[j];//标记当前需要插入的数 i = j - 1;//i作为指针，遍历j之前的数 while (i &gt;= 0 &amp;&amp; a[i] &gt; key) &#123; //开始遍历，如果遍历到的数比key大，就把这个数向后移一位，知道循环完毕或者碰到比key小的数 a[i + 1] = a[i]; i = i - 1; &#125; a[i + 1] = key;//将key插入 &#125;&#125; 希尔排序&emsp;&emsp;在上面的插入排序中，如果原始序列非常大而且是乱序的话，插入排序的速度就会变得非常慢。在插入的过程中，元素只能与相邻的元素交换，于是只能一步一步的移动到最终位置，如果最小元素正好在序列的另一端，那么移动到另外一端就需要N-1次移动。为了加快这一进度，我们需要对插入排序进行改进，这也体现了我们研究初级排序算法的价值。我们让不相邻元素交换以达到局部排序，最后再让元素进行插入排序将局部有序数组排序，这种排序方法叫做希尔排序。&emsp;&emsp;希尔排序的思想是使数组中任意间隔为h的元素有序，称为h有序数组，h有序数组就是h个独立的数组编织在一起形成的数组。从整体数组来看，数组中间每隔h个位置的元素的集合，就是h个独立数组中的一个数组。&emsp;&emsp;对于任意h序列(不是h数组，这里指h的取值序列，最后一个元素是1)，在h很大的时候，我们可以将元素与其间隔非常远的元素进行交换，使得在h取得更小数字的时候排序方便。一般会选取一个初始的h值，N/3也好，N/5也好（N是数组长度)，然后每一次进行h数组进行插入排序之后就递减，一直到1。排序期间，希尔排序会对每一个h独立的子数组进行插入排序，由于子数组是相互独立的，我们只需要将原来插入排序中移动距离由1增加到h，所以希尔排序可以看作是插入排序使用不同增量过程的结果。当h递减到1时，希尔排序就变成了插入排序。&emsp;&emsp;上面几个排序我们都分析了他们的性能，但是对于希尔排序而言，它的性能分析相当复杂，其子数组部分有序成都完全取决于递增序列的选择。而这个选择并不简单，它并非仅仅取决于h的选择，而且与其某些数学性质有关，直到现在也没有证明出来哪一种序列才是最好的选择。&emsp;&emsp;但是事实证明，希尔排序在大型数组的排序中是绝对优于选择排序和插入排序的，并且数组越大，优势越大。只不过对于他性能的认证已经超出我们的范围，唯一能知道的是，它的运行时间达不到平方级别，最坏比较次数也只是与N3/2成正比。 当一个“h有序”的数组按照增幅k排序后，它仍然是“h有序”的。 &emsp;&emsp;希尔排序执行过程如下，h分别取5，3，1。相同颜色代表同一h子数组。&emsp;&emsp;代码如下1234567891011121314151617181920212223#pragma once#include&lt;iostream&gt;using namespace std;void exch(int* a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125;/*希尔排序*/void shellsort(int* a,int length) &#123; int h = 1; while (h &lt; length/3)&#123; h = 3 * h + 1; &#125; while ( h&gt;=1 )&#123; for (int i = h; i &lt; length; i++)&#123; for (int j = i; j &gt;= h&amp;&amp;(a[j]&lt;a[j-h]); j-=h)&#123; exch(a, j, j - h); &#125; &#125; h = h / 3; &#125;&#125; 归并排序&emsp;&emsp;首先对于数组A[1……n] 1.如果n为1，那么数组就是排好序的。2.递归的对A[1到n/2向上取整]的这一部分，以及A[n/2+1向上取整到n]这部分排序3.把排好序的两个表归并 归并过程如图所示实质上就是将两个已排序好对的数组，也就是A[1..N/2]和A[N/2+1..n]，进行归并，得到一个排序好的数组，每一步都是固定的操作与数组长度无关，所以对于总数为n的输入，时间是$\Theta$(n)的。&emsp;&emsp;对于这个递归，我们可以写出一个递推式。 $$T(n) =\begin{cases}Θ(1) &amp; （n=1） \2T(n/2)+ Θ(n) &amp; （n&gt;1）\end{cases}$$ &emsp;&emsp;这是一个树状结构，树的末端时间只有$\Theta$(1)，而树的高度是lgn，叶的节点总数为n，完全扩展的递归树有lgn+1层，每层贡献总代价$\Theta$(n)，如果计算总数就是$\Theta$(n)lgn+$\Theta$(n)，根据渐进的思想，最后结果就是$\Theta$(nlgn)，考虑渐进，他比$\Theta$(n2)快,在数据足够大的情况下，归并排序将优于插入排序，差不多n大于30归并就更快了。下面是对《算法导论》中归并排序的C++实现，该算法使用了自底向上归并，并且使用了辅助数组。123456789101112131415161718192021222324252627282930313233const int N=204800;void Merge(int *arr,int p,int q,int r)&#123; int n1=q-p+1;//左数组长度 int n2=r-q;//右数组长度 int left[n1+1],right[n2+1];//开辟新的左右数组 for(int i = 0; i !=n1; ++i)&#123; left[i]=arr[p+i];//为左数组赋值 &#125; left[n1]=N;//左数组“哨兵” for(int j = 0; j!= n2; ++j)&#123; right[j]=arr[q+j+1];//为右数组赋值 &#125; right[n2]=N;//右数组“哨兵” int i=0,j=0; for(int k = p; k !=r+1; ++k)//将左右数组归并至原数组&#123; if(left[i]&gt;right[j])&#123; arr[k]=right[j]; ++j; &#125;else&#123; arr[k]=left[i]; ++i; &#125; &#125;&#125;void MergeSort(int *arr,int p,int r)&#123; //分治法，将数组分割，将复杂问题化简为数个简单问题 if(p&lt;r)&#123; int q=(p+r)/2;//数组分割标记，中间下标 MergeSort(arr,p,q);//分割左边数组 MergeSort(arr,q+1,r);//分割右边数组 Merge(arr,p,q,r);//进行归并排序 &#125;&#125; 快速排序&emsp;&emsp;快速排序是使用最广泛的排序算法，甚至大部分语言内置的库中使用的排序算法默认使用的就是快速排序。原因很简单，一个是因为它的实现很简单，而且平均性能非常好，一般情况下比其他算法要快得多。快速排序是一种原址排序，而且在长度为n的数组所需的时间和nlgn成正比。&emsp;&emsp;快速排序是基于分治思想的，所以它的算法过程也是经历三个阶段。 1.分解：数组A[p,q]被划分为左子数组A[p..s-1]与右子数组A[s+1..q]，使得A[p..s-1]的每一个元素都小于A[s]，使得A[s+1..q]中的每一个元素都大于A[s]，A[s]所在的位置就是它最终在的位置。2.解决：通过递归的调用快速排序，对子数组A[p..s-1]和A[s+1..q]继续分解，重复执行分解过程，直至不能分解。3.合并：由于我们使用的是原址排序，所以在递归执行到最底层的时候，数组A[p,q]就已经被排序完毕了，并不需要合并操作。 &emsp;&emsp;快速排序最重要的就是数组划分的过程，这一过程实现了原址重排。&emsp;&emsp;过程也比较简单，首先我们选择一个主元，一般为了方便我们都会选第一个或者最后一个，这个主元便是我们数组划分的依据，我们设这个主元为v，设这个数组的上界为hi，下界为lo。为了方便，在这里我们的v就随意的设成第一个元素即A[lo]，我们使用标记i、j，分别从A[lo+1]和A[hi]开始（也就是两端，不从A[lo]开始是因为A[lo+1]是主元），逐步从中间靠拢。如果出现A[i] &gt; v,i停止向后靠拢，出现A[j] &lt; v,j停止向前靠拢，二者同时停止时，就将A[i]与A[j]互换位置，然后继续向中间靠拢，重复上面的操作，直到i与j相遇，然后将v放在这个位置，这是就满足了主元前面都比主元小，主元后面都比主元大，同时主元也为更深层次的数组划分提供依据，而且这个数组被遍历了一次，所以一次划分的时间复杂度是$\Theta$(n) （n = hi - lo + 1）。下面是划分过程。 &emsp;&emsp;注意，这里只是提供一种快速排序的数组划分方法，这个方法并不惟一，不同教材上写的也不尽相同，但是划分思路是一样。 &emsp;&emsp;在性能方面，快速排序的运行时间主要取决于数组划分是否平衡，是否平衡划分又取决于用于划分的元素，即主元。如果划分平衡的话，它的性能相当于归并排序，如果不平衡的话，它的效率就接近于插入排序了。&emsp;&emsp;如果是在最坏情况下划分的话，即一子数组被分到了n-1个元素，一个数组被分到了0个元素，这种划分是最不均衡的。那我们就假设每一次划分都会遇见这一种情况，划分操作的复杂度为$\Theta$(n)。一个数组为0时，递归会直接返回，所以 T(0) = $\Theta$(1)。我们可以得到算法的递归式。 T(n) = T(n - 1) + T(0) + $\Theta$(n) = T(n - 1) + $\Theta$(n) &emsp;&emsp;我们将每一层的代价累加，就能得到一个算术级数，其结果是$\Theta$(n2)。所以在最坏情况下，快速排序的效率和插入排序一样，就算输入数组已经完全有序，它的效率还是$\Theta$(n2)，而这时候插入排序已经就是$\Theta$(n)了。 &emsp;&emsp;再看看最好的情况，最平衡的划分就是把主元去掉然后对半分了，两个子数组的大小都不会大于n/2。一个子数组规模为$\lfloor$n/2$\rfloor$，而另一个子数组的规模就是$\lceil$n/2$\rceil$-1.我们可以得到一个递归式。 T(n) = 2T(n/2) + $\Theta$(n) &emsp;&emsp;根据主定理，这个递归式的解是T(n) = $\Theta$(nlgn)。这样的划分可以使渐进时间更快。&emsp;&emsp;快速排序算法被广泛使用的原因就是它的平均运行时间总是接近于它的最好情况的。假设产生一个9:1的划分，那么它的递归式就是。 T(n) = T(9n/10) + T(n/10) + cn我们显示的写出$\Theta$(n)所隐含的常数c &emsp;&emsp;如果是一棵树的话，这棵树的每一层代价都是cn。这棵树在深度为log10n = $\Theta$(nlgn)地方就能达到递归边界，而我们的递归的终止是在深度为log10/9n = $\Theta$(lgn)的地方。在达到第一个递归边界与递归终止之前，每层代价至多为cn。所以说它的总代价就是O(nlgn)。即使是9:1这种非常不规则的划分，它的运行时间居然和最好运行时间是一样的。事实上，即便是99：1，它的时间复杂度还是这样。 任何一种常数比例划分都会产生深度为$\Theta$(lgn)都递归树，每一层代价都是O(n)。所以只要划分是常数比例，它的运行时间总是O(nlgn)。 &emsp;&emsp;下面给出快速排序的实现，这个实现是由E.W.Dijkstra（就是那个图论中求最短路径的Dijkstra算法的发明者）解出的三项切分的快速排序，我们在小于和大于的基础上引进一个等于，解决了重复排序的问题，使得其在重复元素很多的应用中更加快速。123456789101112131415161718192021222324252627#pragma once#include&lt;iostream&gt;using namespace std;void exch(int* a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125;/*三向切分快速排序*/void quicksort(int*a, int lo, int hi) &#123; if (hi &lt;= lo)return; int lt = lo, i = lo + 1, gt = hi; int v = a[lo]; while (i &lt;= gt)&#123; if (a[i] &lt; v) &#123; exch(a, lt++, i++); &#125; else if(a[i] &gt; v)&#123; exch(a, i, gt++); &#125; else&#123; i++; &#125; &#125; quicksort(a, lo, lt - 1); quicksort(a, gt+1, hi);&#125; 堆排序&emsp;&emsp;首先来说一下堆是个什么东西，它也叫二叉堆，一般用数组实现，从逻辑结构上讲类似一棵完全二叉树，每个节点对应数组中的元素，除了最后一层之外，这个树是完全充满的。在数组中，我们可以通过一些简单的计算来算出一棵树的父节点以及左右叶子结点。 int parent(int i) { return i / 2;}int left(int i) { return 2 i;}int right(int i) { return 2 i + 1;} &emsp;&emsp;二叉堆分两种，一种叫最大堆，一种叫最小堆，他们的共同点是都满足堆的性质，即是一个完全二叉树。最大堆中，对于某一个节点i来说，它的大小必须不能大于它的父节点。而在最小堆中，任意一个节点不能小于它的父节点。直观来讲，在层序上，从下往上呈现上升（下降）的趋势，根节点是最大（小）的元素。既然堆是一种完全二叉树，那么n个节点的完全二叉树的高度就是$\Theta$(lgn)，基本操作时间与高度成正比，所以时间复杂度也是O(lgn)。&emsp;&emsp;由于在堆排序中可能会有元素变动之后，堆就不符合最大堆的性质，或者整个数组输入的时候就不符合最大堆的性质。这个时候我们需要让某些元素在堆中逐级下降，使其能够满足最大堆的性质，这个过程就是堆的维护。堆的维护是一个递归的过程，每次递归的操作，就是将父节点i与其左右节点相比较，如果i是最大的，那么这个子树必然满足最大堆的性质，如果最大的点不是i，那么i节点与那个最大的元素交换，递归完毕之后，就能够形成一个最大堆。这一过程中，它的时间代价有调整父节点与其左右关系节点的代价，是$\Theta$(1)。再算上时间代价每个孩子的子树大小至多为2n/3（底层恰好半满就是最坏情况）。我们可以得到一个递归式。 T(n) = T(2n/3) + $\Theta$(1) &emsp;&emsp;使用主定理解一下，能得到T(n) = O(lgn)。所以说，对于一个堆的维护来说，它的时间复杂度与其树的高度有关。&emsp;&emsp;接下来要执行建堆过程，数组元素从$\lfloor$n/2$\rfloor$+1到n都是叶子结点，而叶子结点所形成的堆只有一个元素，不需要调整，那么我们就需要对非叶子结点进行最大堆的调整。我们从$\lfloor$n/2$\rfloor$开始一直到1，逐个调用堆的维护算法，使其形成最大堆。由于过程复杂，时间复杂度的推导就不再写了，把无序数组构建为一个最大堆的时间是线行的，也就是O(n)。&emsp;&emsp;在有了堆的维护和堆的调整之后，就可以进行堆排序算法了，它的本质是一种选择排序。首先将数组初始化为最大堆，这时候根节点就是最大的，然后将根节点与最后一个节点交换，此时最后位置这个节点就需要被排除出堆了，因为他目前已经到了它最终所在的位置（这一操作可以通过n-1来实现）。这个时候的堆可能已经不满足最大对的性质了，所以要进行一次调整，使其继续满足最大堆的特性。然后重复上面的操作，直到最后就剩一个节点。此时这个数组已经被排序了。这一过程非其实就是一个选择的过程，通过最大堆找到堆中最大的元素，将其挑出，然后再次调整为堆大堆，就能够挑出第二大的元素，直到最后就能将所有元素排序，下面是堆排序的过程，并没有排完，但是后面的过程是一样的。 &emsp;&emsp;堆排序过程中，每一次调用建堆的时间复杂度是O(n)，n-1次调用堆维护，每次时间为O(lgn)，所以对排序的时间复杂度为O(nlgn)。下面是实现。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#pragma once#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int parent(int i) &#123; return i / 2;&#125;int left(int i) &#123; return 2 * i;&#125;int right(int i) &#123; return 2 * i + 1;&#125;void Maxheapify(vector&lt;int&gt;&amp; A, int i, int length) &#123; int largest; int l = left(i);// 左子节点坐标 int r = right(i);// 右子节点坐标 if (l&lt;length &amp;&amp; A[l]&gt;A[i]) &#123;//用两个判断筛选当前节点与左右子节点的最大值 largest = l; &#125; else &#123; largest = i; &#125; if (r&lt;length &amp;&amp; A[r]&gt;A[largest]) &#123; largest = r; &#125; if (largest != i) &#123;//当前节点与除本身以外的最大值交换 int k = A[i]; A[i] = A[largest]; A[largest] = k; Maxheapify(A, largest, length);//继续向下重复此操作 &#125;&#125;void buildmaxheap(vector&lt;int&gt;&amp; A) &#123;//建立最大堆 int length = A.size(); for (int i = A.size() / 2; i &gt;= 1; i--)&#123; Maxheapify(A, i, length); &#125;&#125;/*堆排序*/void heapsort(vector&lt;int&gt;&amp; A) &#123; buildmaxheap(A); int length = A.size(); for (int i = length - 1; i &gt;= 2; i--)&#123; int k = A[i]; A[i] = A[1]; A[1] = k; length -= 1; Maxheapify(A, 1, length); &#125;&#125; 基数排序&emsp;&emsp;这是一种原来在卡片机上的一种算法，这种算法是按照每一位进行排序的，每一位的取值范围就是0-9这10个数字，每一个数字都代表一个容器。如果按照常规想法都话，一般从在高位开始排序，先按照最高位将数字插入0-9之间的容器中，然后将每个容器中的数据按照第二位再进行这样的排序，这是一个递归操作，直到最后。但是这么做的话一次只能排一个容器里的数据，当一个容器在排序的时候，其他容器都是未使用状态，多出来了许多不必要的临时储存空间，所以这种方式在很久以前内存紧张的时候并不是一个好的算法。&emsp;&emsp;而基数排序则是选择从最低有效位开始进行排序，然后合并到一起。从第一位开始，所有第一位是0的数字排在所有第一位是1的数字之前，所有第一位是1的数字排在所有第一位是2的数字之前……然后对第二位进行重复操作，直到最高位，过程如图所示。&emsp;&emsp;基数排序的算法就是这么直观，但是它的算法效率又是怎样的呢？ 给定n个d位数，其中每一个数位有k个取值，如果基数排序使用的稳定排序方法耗时$\Theta$(n+k)，那么他就可以在$\Theta$(d(n+k))时间内排好序。 &emsp;&emsp;当每一位数字都在0到k-1之间，并且这个k不太大时，可以使用计数排序（不想在写这个计数排序了，这是一个区间范围内的整数排序，它的时间代价是$\Theta$(n+k)，十进制的基数排序中正好k就等于10，很适合采用计数排序对每一位进行排序。对于n个d位数字来说，每一轮排序耗费$\Theta$(n+k)，一共有d轮，所以基数排序总排序时间为$\Theta$(d(n+k))。 对于给定n个b位数的和任何正整数r&lt;=b，如果基数排序使用的稳定排序算法对数据取值区间是0到k到输入进行排序耗时$\Theta$(n+k)，那么他就可以在$\Theta$((b/r)(n+2r))将这些数排好序。 &emsp;&emsp;证明就不写了，脑壳疼……&emsp;&emsp;至于这个选择用不用，我觉得还是别用，它的运行实现可能看上去比其他排序好一些，但是在渐进记号后面的常数因子却大的多，并且他没有快速排序那样可以有效使用硬件缓存，而且他也不是原址排序。具体实现如下。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#pragma once#include &lt;iostream&gt;using namespace std;/*基数排序Input: 数组A[l,h]； 数组中最大元素的位数d，例如最大数为999，则d为3； 进制数k，如果是10进制数，k为10；Output:排序好的数组；Others：对数字1234来说，预定第0位为4，第1位为3，依次类推；*/bool radixsort(int A[], int l, int h, int d, int k) &#123; if (NULL == A || l &gt; h) return false; int size = h - l + 1; int* counts = new int[k];//辅助数据 int* temp = new int[size];//用于存储重新排序的数组 int index; int pval = 1; //依次处理不同的位 for (int i = 0; i &lt; d; i++) &#123; //counts数组清零 for (int j = 0; j &lt; k; j++) counts[j] = 0; for (int j = l; j &lt;= h; j++) &#123; index = (int)(A[j] / pval) % k; counts[index]++; &#125; //计算累加频数 for (int j = 1; j &lt; k; j++) counts[j] = counts[j] + counts[j - 1]; //使用倒数第i+1位数对A进行排序 for (int j = h; j &gt;= l; j--) &#123; index = (int)(A[j] / pval) % k; temp[counts[index] - 1] = A[j]; counts[index]--; &#125; //将按第i为数排序后的结果保存回数组A中 for (int j = 0; j &lt; size; j++) A[j + l] = temp[j]; //更新pval pval = pval * k; &#125; delete[] counts; delete[] temp;&#125; 算法比较&emsp;&emsp;这八种排序算法具体该选择哪一种，很大程度上取决于实际应用场景。下面用一张表来总结以上九种（快速排序分为普通的和三向切分的）排序算法。算法|是否稳定|是否为原址排序|时间复杂度|空间复杂度|备注– |:–:|–:|–:|–:|–:选择排序|否|是|N2|1|插入排序|是|是|介于N和N2之间|1|与元素初始状态有关希尔排序|否|是|近似于NlgN和N6/5|1|目前没有证出来快速排序|否|是|NlgN|lgN|有大概率取得最好运行状态三向切分快速排序|否|是|介于N和NlgN之间|lgN|同时取决于概率和元素分布归并排序|是|否|NlgN|N堆排序|否|是|NlgN|1基数排序|是|否|d(N+k)|(N+k)|受隐藏常数因子影响太大]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[禹州实训-宠物医院-后端]]></title>
    <url>%2F2019%2F05%2F22%2F%E7%A6%B9%E5%B7%9E%E5%AE%9E%E8%AE%AD-%E5%AE%A0%E7%89%A9%E5%8C%BB%E9%99%A2-%E5%90%8E%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[禹州实训-宠物医院(后端)&emsp;&emsp;今年实训项目是一个只有增删改查的管理系统，没啥难度，后端基本都是体力活，所以SpringBoot加Spring-data-jpa写这个最爽了，前端打算使用Angular和Angular Material构建一下(不想用jsp那种破烂了)，前端东西有点多，所以分开记录(前端太无聊，鸽了)。so，这就是一个前后端分离的项目了，后端目前就只有一个写接口的任务。 项目分析&emsp;&emsp;没啥好分析的，照着老师给的文档来吧。 “爱心”宠物诊所的职员需要使用系统提供的如下功能：浏览诊所的兽医以及他们的专业特长；浏览宠物的主人（即诊所的客户）的相关信息；更新宠物的主人的相关信息；向系统中增加一个新客户；浏览宠物的相关信息；更新宠物的相关信息；向系统中增加一个新宠物；浏览宠物的访问历史记录；向宠物的访问历史记录添加一次访问；此外，诊所的职员在使用系统提供的上述功能之前需要进行登录。当职员不需要使用系统的上述功能时，也可退出系统。 数据库建立&emsp;&emsp;根据提供的ER图，一共需要七张表，其中一张表独立，两张表有多对多关系，四张表相互之间有一对多或者多对一关系，但是经过考虑我决定两张具有多对多关系的表在数据库中依然保持其多对多关系，而另外四张需要外键连接的表在数据库中不再保持外键关系，所有外键都在Service层进行处理，具体关系如图所示。 项目配置&emsp;&emsp;SpringBoot怎么创建就不废话了，先说一下主要的依赖。由于不用写jsp，所有就不需要使用jsp的依赖，那么还剩下的依赖有。 数据库驱动数据源SpringMVCSpring-data-jpa 这四个依赖就够了，所以pox.xml里面就写上1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.edu&lt;/groupId&gt; &lt;artifactId&gt;zzuli&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;zzuli&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;11&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--springmvc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--数据源--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; &emsp;&emsp;依赖写完之后，就开始写配置文件，配置文件还是使用yaml格式123456789spring: jpa: database: mysql properties: hibernate: hbm2ddl: auto: update show_sql: true format_sql: true 正常情况下配置文件里应该还有数据源配置，但是不知道为什么这一回写在配置文件中没有用了，所以单独写一个配置类Config.java123456789101112131415161718192021package cn.edu.zzuli;import com.alibaba.druid.pool.DruidDataSource;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @author 程佩 * @datatime 2019/5/14 11:04 */@Configurationpublic class Config &#123; @Bean public DruidDataSource druidDataSource() &#123; //Druid 数据源配置 DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName("com.mysql.cj.jdbc.Driver"); dataSource.setUrl("jdbc:mysql://你的ip/pet?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;allowPublicKeyRetrieval=true");//mysql8.0新写法 dataSource.setUsername("root"); dataSource.setPassword("123456”); //初始连接数(默认值0) dataSource.setInitialSize(8); // 至此，所有项目需要的依赖和配置已经设置完毕。 Entity类设计&emsp;&emsp;这些类中包含了所有数据库需要的实体类，对应那七张表。本来这个包准备起名叫domain呢，很大程度上成了一个行业的约束，domain就代表了数据库映射。但是前几天看了一篇文章指出，在Java传统web开发中，通常我们写的所谓domain都是一些贫血模型，并没有领域模型的行为，所以这些类都只能是一个entity对象，并非领域对象，所以我把包名改成了entity。 Employ类&emsp;&emsp;这个类中包含员工信息，里面有一个主键自增长的id，一个名字，一个密码，下面是代码，get和set方法省略。12345678910111213141516171819202122package cn.edu.zzuli.domain;import javax.persistence.*;import java.io.Serializable;/** * @author 程佩 * @datatime 2019/5/14 10:12 */@Entity@Table(name = "Employ")public class Employ implements Serializable &#123; @Id//主键 @GeneratedValue(strategy = GenerationType.IDENTITY)//自增长 @Column(name = "id") private Integer id; @Column(name = "name") private String name; @Column(name = "password") private String password; @Column(name = "isactive") private Integer isactive;//这个目前没用 /*getter and setter*/&#125; &emsp;&emsp;全部采用注解的方式，下面所有的表都是这样。 Specialties与Vets类设计&emsp;&emsp;这两个类分别是特长类和兽医类，因为存在多对多关系，所以放在一起。其中兽医类维护两个实体类之间的关系。123456789101112131415161718192021222324package cn.edu.zzuli.domain;import com.fasterxml.jackson.annotation.JsonIgnoreProperties;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 14:30 */@Entity@Table(name = "vets")public class Vets &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @JsonIgnoreProperties(value = "vetsList") @ManyToMany//多对多映射 @JoinTable(name = "vets_specialties",joinColumns = @JoinColumn(name = "vets_id"), inverseJoinColumns = @JoinColumn(name = "specialties_id"))//外键关联 private List&lt;Specialties&gt; specialtiesList; /*getter and setter*/&#125; &emsp;&emsp;主要记录一下@JsonIgnoreProperties(value = &quot;vetsList&quot;)这一句，因为不加这一句的话，在多对多查询中，两个类中的成员变量都会有对方的列表，这样会造成无限循环查询，加上这一句是为了避免内层再次查询，相应的，特长类中也会有这一行注解，下面是特长类的代码。12345678910111213141516171819202122package cn.edu.zzuli.domain;import com.fasterxml.jackson.annotation.JsonIgnoreProperties;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 14:32 */@Entity@Table(name = "specialties")public class Specialties &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @JsonIgnoreProperties(value = "specialtiesList") @ManyToMany(mappedBy = "specialtiesList") private List&lt;Vets&gt; vetsList; /*getter and setter*/&#125; Owner、Pet、Type、Visits类设计&emsp;&emsp;这四张表分别对应主人信息、宠物信息、宠物类型、病历信息，因其存在外键关系，所以这四个表放一起讲。&emsp;&emsp;Owner之中有主键、主人姓名、电话、家庭住址四个信息，所以实体类很简单。12345678910111213141516171819202122package cn.edu.zzuli.domain;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 11:18 */@Entity@Table(name = "owner")public class Owner &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @Column(name = "address") private String address; @Column(name = "telephone") private String telephone; /*getter and setter*/&#125; &emsp;&emsp;Pet比较复杂，因为他跟两个表之间有这多对一关系，所以要记录关联信息，为了方便查询，我不止关联了主键信息，还关联了一些必要的名称信息。123456789101112131415161718192021222324252627package cn.edu.zzuli.domain;import javax.persistence.*;import java.util.Date;/** * @author 程佩 * @datatime 2019/5/14 11:40 */@Entity@Table(name = "pet")public class Pet &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @Column(name = "brith_date") private String brithDate; @Column(name = "type_id")//与type关联 private Integer typeId; @Column(name = "type_name") private String typeName; @Column(name = "ownerId")//与owner关联 private Integer ownerId; @Column(name = "owner_name") private String ownerName; /*getter and setter*/ &emsp;&emsp;Type十分简单，除了主键之外只有一个name信息。123456789101112131415161718package cn.edu.zzuli.domain;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 11:35 */@Entity@Table(name = "type")public class Type &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; /*getter and setter*/&#125; &emsp;&emsp;Visits和Pet之间存在多对一关系。12345678910111213141516171819202122package cn.edu.zzuli.domain;import javax.persistence.*;import javax.xml.crypto.Data;/** * @author 程佩 * @datatime 2019/5/14 14:22 */@Entity@Table(name = "visits")public class Visits &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "visit_date") private String visitDate; @Column(name = "description") private String description; @JoinColumn(name = "pet_id") private Integer petId; /*getter and setter*/&#125; 项目将一启动，这七个表就会被创建了。 JPA设计&emsp;&emsp;JPA使用非常简单，只要继承JpaRepository接口就能使用一套已经写好的增删改查接口，但是自动生成可能无法满足自己的需求，所以需要在里面再加几个方法。(由于急着玩游戏学习，就不写基类JPA，反正就继承这一个接口，分页什么的扔前端了) PetJPA&emsp;&emsp;再这个接口中我想添加根据类型ID查询宠物接口、根据主人ID查询宠物接口，根据主人的ID更改所有属于该主人的宠物中的主人姓名的接口，在没有使用QueryDSL的时候，我觉得最好在接口中自己用HQL或者原生SQL编写接口、需要注意的是在修改和删除操作的时候要加上事务。修改操作的时候要加@Modifying注解。123456789101112131415161718192021package cn.edu.zzuli.jpa;import cn.edu.zzuli.domain.Pet;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Modifying;import org.springframework.data.jpa.repository.Query;import javax.transaction.Transactional;import java.util.ArrayList;/** * @author 程佩 * @datatime 2019/5/14 14:47 */public interface PetJPA extends JpaRepository&lt;Pet,Integer&gt; &#123; @Query("select pet from Pet pet where pet.typeId = ?1") public ArrayList&lt;Pet&gt; getPetByType(Integer id); @Query("select pet from Pet pet where pet.ownerId = ?1") public ArrayList&lt;Pet&gt; getPetByOwner(Integer id); @Transactional @Modifying @Query("update Pet pet set pet.ownerName = ?1 where pet.ownerId = ?2") public void updatePetByOwner(String name, Integer id);&#125; 下面只写添加的方法，不再写整个接口了。 OwnerJPA&emsp;&emsp;OwnerJPA只需要添加一个模糊查询接口。12@Query(value = "select o from Owner o where o.name LIKE CONCAT('%',?1,'%') ")public ArrayList&lt;Owner&gt; selectByNamelike(String name); EmployJPA&emsp;&emsp;EmployJPA里面只需要一个根据姓名查询，但是这里就不需要写查询语句了，jpa接口中是可以根据接口名称来自动实现接口，比如下面这个写法，就能自动解析为查询语句，不过只适用于简单查询，复杂查询写法太过繁琐。1Employ findByName(String name); &emsp;&emsp;SpecialtiesJPA和VetsJPA都有这个方法，就不重复写。 VisitsJPA&emsp;&emsp;VisitsJPA里面需要一个根据宠物ID查询宠物信息的方法，如下所示。12@Query("select v from Visits v where v.petId = ?1") public ArrayList&lt;Visits&gt; getListByPet(Integer id); &emsp;&emsp;数据库操作已经编写完毕，接下来就是编写Service接口了。 Service接口设计&emsp;&emsp;关于Service层的设计也很简单，俗话说代码就是最好的注释，所以这里我并没有写太多注释，只放接口（从接口名完全可以看出来接口的功能），实现类放在github上。 EmployService123456789/** * @author 程佩 * @datatime 2019/5/14 16:28 */public interface EmployService &#123; public Employ SearchEmploy(Employ employ); public boolean Verify(Employ employ); public Employ Logon(Employ employ);&#125; OwnerService12345678910111213/** * @author 程佩 * @datatime 2019/5/14 18:30 */public interface OwnerService &#123; public ArrayList&lt;Owner&gt; ownerList(); public ArrayList&lt;Owner&gt; searchByName(String name);//模糊查询 public Optional&lt;Owner&gt; searchById(Integer id); public Owner addOwner(Owner owner); public Owner updataOwner(Owner owner); public Owner deleteOwner(Owner owner); public void deleteOwnerById(Integer id);&#125; PetService12345678910111213/** * @author 程佩 * @datatime 2019/5/14 20:45 */public interface PetService &#123;public Pet addPet(Pet pet);public void deletePet(Integer id);public Pet uptataPet(Pet pet);public ArrayList&lt;Pet&gt;getPetList();public Optional&lt;Pet&gt; getPetById(Integer id);public ArrayList&lt;Pet&gt; getPetByType(Integer id);public ArrayList&lt;Pet&gt; getPetByOwner(Integer id);&#125; TypeService12345678910/** * @author 程佩 * @datatime 2019/5/14 20:19 */public interface TypeService &#123; public Type add(Type type); public void delete(Integer id); public Type updata(Type type); public ArrayList&lt;Type&gt; getlist();&#125; Vets_SpService&emsp;&emsp;这个接口要说明一下，因为Specialties与Vets两个表是多对多关系，操作时通常为关联查询，为了方便起见，就把他们两个的服务接口放在一起。1234567891011/** * @author 程佩 * @datatime 2019/5/15 11:02 */public interface Vets_SpService &#123; public ArrayList&lt;Vets&gt; getVetsList(); public ArrayList&lt;Specialties&gt; getSpecialtiesList(); public void insert(SPSet set) throws Exception; public Optional&lt;Vets&gt; getVets(Integer id); public Optional&lt;Specialties&gt; getSpecialties(Integer id);&#125; VisitsService123456789101112/** * @author 程佩 * @datatime 2019/5/15 9:22 */public interface VisitsService &#123; public Visits add(Visits visits); public void delete(Integer id); public Visits updata(Visits visits); public ArrayList&lt;Visits&gt; getlist(); public ArrayList&lt;Visits&gt; getListByPet(Integer id); public Optional&lt;Visits&gt; getVisitById(Integer id);&#125; Optional&emsp;&emsp;在上面的接口中，可以看见很多返回Optional的接口，因为Spring-data-jpa在进行单个对象查询的时候不会只返回对象，而是返回了一个Optional类，这是一个再jdk8中引入的很有趣的特性，它的存在就是为了解决令许多java程序员头疼的的空指针异常。本质上，这是一个包含有可选值的包装类，这意味着 Optional 类既可以含有对象也可以为空。可以说是java迈向函数式编程的很强劲的一步，不过在这里不是重点，不多说。 Controller层设计&emsp;&emsp;这一层是直接和前端进行交互的，比较重要。但是这个项目本身并没有什么东西，所以Controller层设计还是很简单的。 EmployController12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * @author 程佩 * @datatime 2019/5/14 16:51 */@RestController@RequestMapping(value = "/employ")public class EmployController &#123; @Autowired private EmployService employService; /** *@Author:程佩 *@Description:验证用户信息 *@Date:2019/5/14_17:19 */ @CrossOrigin @RequestMapping(value = "verify",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; Verify(@RequestBody Employ employ)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); boolean flag = employService.Verify(employ); if (flag)&#123; map.put("state","1"); map.put("message","登陆成功！"); &#125;else &#123; map.put("state","0"); map.put("message","密码或账号不正确！"); &#125; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; /** *@Author:程佩 *@Description:注册账号 *@Date:2019/5/14_17:23 */ @CrossOrigin @RequestMapping(value = "logon",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; LogOn(@RequestBody Employ employ)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); Employ searchEmploy = employService.SearchEmploy(employ); if (searchEmploy == null)&#123; employService.Logon(employ); map.put("state","1"); map.put("message","注册成功！"); &#125;else &#123; map.put("state","0"); map.put("message","账号已存在！"); &#125; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125;&#125; &emsp;&emsp;说一下这个ResponseEntity，其实本来直接使用HashMap也行，因为做post提交这种非幂等操作，每次操作结果都不一样，反正返回一个状态让前端知道后面操作是个什么情况就行，但是我在浏览Spring文档的时候发现了这么一个类，仔细看了看才知道它可以返回一整个http响应，包含状态码、头部信息以及相应体内容，显然比只返回一个json串更合理，索性就用它了，具体怎么用在代码里体现的也有。&emsp;&emsp;再说一下@CrossOrigin，这个注解可能在以前很少遇到，因为之前的jsp开发，静态资源和后端代码耦合比较严重，都放在一个服务器上。但是在前后端分离的情况下，由于不在一个服务器上，就会产生跨域问题，所谓跨域也就是浏览器对于同源策略（就是同一个域）的限制，浏览器会阻止服务器（域）之间的交流，协议、域名、端口其中之一不同就是跨域，一种解决办法就是使用nginx做反向代理，强行使两个服务器在同一个域中，但是这个有点麻烦（这个项目太小了，没有必要）。另一个方法就是Spring中给的@CrossOrigin注解，原理就是向请求中添加Access-Control-Allow-Origin等信息解决跨域问题。&emsp;&emsp;下面的Controller类如果没有特别的地方就不再做详细说明，只贴代码，因为方法名已经把该接口的作用描述得够清楚类了，多写也无益。 OwnerController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * @author 程佩 * @datatime 2019/5/14 19:06 */@RestController@RequestMapping(value = "/owner")public class OwnerController &#123; @Autowired private OwnerService ownerService; @CrossOrigin @RequestMapping(value = "getlist",method = RequestMethod.GET) public ArrayList&lt;Owner&gt; getOwnerList()&#123; return ownerService.ownerList(); &#125; @RequestMapping(value = "getlistbyname",method = RequestMethod.GET) public ArrayList&lt;Owner&gt; getOwnerListByName(String name)&#123; return ownerService.searchByName(name); &#125; @CrossOrigin @RequestMapping(value = "getbyid/&#123;id&#125;" ,method = RequestMethod.GET) public Optional&lt;Owner&gt; getOwnerById(@PathVariable("id") Integer id)&#123; return ownerService.searchById(id); &#125; @CrossOrigin @RequestMapping(value = "add",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; addOwner(@RequestBody Owner owner)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.addOwner(owner); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "updata",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updateOwner(@RequestBody Owner owner)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.updataOwner(owner); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "delete",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; deleteOwner(@RequestBody Owner owner)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.deleteOwner(owner); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "deletebyid/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; deleteOwnerById(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.deleteOwnerById(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125;&#125; PetController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * @author 程佩 * @datatime 2019/5/15 8:39 */@RestController@RequestMapping(value = "/pet")public class PetController &#123; @Autowired private PetService petService; @CrossOrigin @RequestMapping(value = "add",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; addPet(@RequestBody Pet pet)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; petService.addPet(pet); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "delete/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; deletePet(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; petService.deletePet(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "updata",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updataPet(@RequestBody Pet pet)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; petService.uptataPet(pet); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "getlist" ,method = RequestMethod.GET) public ArrayList&lt;Pet&gt; getPetList()&#123; return petService.getPetList(); &#125; @CrossOrigin @RequestMapping(value = "get/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Pet&gt; getPetListById(@PathVariable("id") Integer id)&#123; return petService.getPetById(id); &#125; @CrossOrigin @RequestMapping(value = "getbyowner/&#123;id&#125;",method = RequestMethod.GET) public ArrayList&lt;Pet&gt; getPetListByOwner(@PathVariable("id") Integer id)&#123; return petService.getPetByOwner(id); &#125; @CrossOrigin @RequestMapping(value = "getbytype/&#123;id&#125;") public ArrayList&lt;Pet&gt; getPetListByType(@PathVariable("id") Integer id)&#123; return petService.getPetByType(id); &#125;&#125; TypeController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @author 程佩 * @datatime 2019/5/14 20:29 */@RestControllerpublic class TypeController &#123; @Autowired private TypeService typeService; @RequestMapping(value = "/type",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; add(@RequestBody Type type)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; typeService.add(type); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "/type/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; delete(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; typeService.delete(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "/type",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updata(@RequestBody Type type)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; typeService.updata(type); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/type",method = RequestMethod.GET) public ArrayList&lt;Type&gt; get()&#123; return typeService.getlist(); &#125;&#125; &emsp;&emsp;关于TypeController(宠物类型控制器)，我写了一个符合rest(表现层状态转换)规范的接口，通过一个统一的接口/type用来通讯，使用GET、POST、PUT、DELETE对资源进行交互。 Vets_SpController1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author 程佩 * @datatime 2019/5/15 11:39 */@RestController@RequestMapping(value = "/vetsp")public class Vets_SpController &#123; @Autowired private Vets_SpService service; @CrossOrigin @RequestMapping(value = "getvetslist",method = RequestMethod.GET) public ArrayList&lt;Vets&gt; getVetsList()&#123; return service.getVetsList(); &#125; @RequestMapping(value = "getsplist",method = RequestMethod.GET) public ArrayList&lt;Specialties&gt; getSpecialtiesList()&#123; return service.getSpecialtiesList(); &#125; @RequestMapping(value = "getvetbyid/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Vets&gt; getVetsById(@PathVariable("id") Integer id)&#123; return service.getVets(id); &#125; @RequestMapping(value = "getspbyid/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Specialties&gt; getSpById(@PathVariable("id") Integer id)&#123; return service.getSpecialties(id); &#125; @CrossOrigin @RequestMapping(value = "/insert",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; insert(@RequestBody SPSet set)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; System.out.println(set.getVname()+" "+set.getSname()); service.insert(set); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125;&#125; VisitsController123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@RestController@RequestMapping(value = "/visits")public class VisitsController &#123; @Autowired private VisitsService visitsService; @CrossOrigin @RequestMapping(value = "/add",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; add(@RequestBody Visits visits)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; visitsService.add(visits); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/delete/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; delete(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; visitsService.delete(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/updata",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updata(@RequestBody Visits visits)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; visitsService.updata(visits); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/getlist",method = RequestMethod.GET) public ArrayList&lt;Visits&gt; getlist()&#123; return visitsService.getlist(); &#125; @CrossOrigin @RequestMapping(value = "/getlistbypet/&#123;id&#125;",method = RequestMethod.GET) public ArrayList&lt;Visits&gt; getlist(@PathVariable("id") Integer id)&#123; return visitsService.getListByPet(id); &#125; @CrossOrigin @RequestMapping(value = "/getbyid/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Visits&gt; getbyid(@PathVariable("id") Integer id)&#123; return visitsService.getVisitById(id); &#125;&#125; 结尾&emsp;&emsp;总体来说这就是一篇流水账……源代码开放在github上-&gt;地址。还有，这回禹州的网没以前好，打星际还凑合。餐厅里唯一的一家杂炣没了，比较难受。银梅可乐挺好喝。没了。]]></content>
      <categories>
        <category>禹州实训</category>
      </categories>
      <tags>
        <tag>禹州实训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker部署静态网页]]></title>
    <url>%2F2019%2F04%2F28%2F%E4%BD%BF%E7%94%A8docker%E9%83%A8%E7%BD%B2%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[使用docker部署静态网页&emsp;&emsp;Docker是一个使用Go语言开发的容器引擎，可以将应用以及依赖环境打包进容器中，容器是可以移植到任何操作系统上(一般是Linux)，容器与容器之间是完全隔离的，容器之间不会有任何接口。&emsp;&emsp;别的也就不多说了，反正网上一大片讲Docker的好处，这次笔记主要是记录一下使用Docker部署一下一个简单的静态网页，开发环境是MacOS，Docker已下载好，既然是部署静态网页，那么首选必然是Nginx，这个服务器既轻，并发量又大。 1. 获取Nginx&emsp;&emsp;Docker官方已经准备好了十万多个模板镜像，就在模板镜像库里，Nginx必然也会在模板镜像库中存在，所以没必要自己下载Nginx去安装，配置好Docker之后直接执行指令。 docker pull nginx 简单粗暴，直接从镜像库中拉出Nginx。 2. 启动镜像&emsp;&emsp;依旧只有一条指令。 docker run -p 8080:80 -d nginx &emsp;&emsp;这里使用的docker run命令相当于先执行了docker create指令，后执行了docker start指令，可以说是先创建再执行的指令。&emsp;&emsp;要注意的是，里面的80:80代表将虚拟机(镜像)的80端口(Nignx使用的是80端口)映射到本机的8080端口，也可以映射到其他端口，冒号前面是本机端口，冒号后面是虚拟机端口，这个参数是必须有的(没有的话访问不到虚拟机)。&emsp;&emsp;可以使用docker ps指令去查看目前正在运行的虚拟机，如下图所示。&emsp;&emsp;图中显示了正在运行的虚拟机，可以看见主机的8080端口映射进了Docker的Nginx虚拟机的80端口，但是还可以看见主机的80端口也被映射了进来，这个是另一个东西，一会会说到。这回已经可以在浏览器访问本机的8080端口了，不出意外的话就可以看到熟悉的welcome to nginx!我的虚拟机里已经扔进去东西了，所以就不截图了。 3. 部署静态文件&emsp;&emsp;有两种方法把静态文件扔进容器中，一个是通过命令行。 docker cp index.html ec8aca4bc4f2://usr/share/nginx/html 利用cp指令将当前已经写好的文件扔进容器中nginx的html文件中，ec8aca4bc4f2是容器的一个标签，可以在刚才查看容器的地方找到。执行 docker exec -it ec8aca4bc4f2 bash 可以将命令行转到容器的伪终端中，能够查看容器中是否存在刚刚拷贝进去的静态文件。退出的时候用Ctrl+P+Q，不然退出时会把容器关闭。(这个方法网上说管用，但是不知道为啥我这里没用)&emsp;&emsp;另外一个方法就是采用文件映射，使用 docker run -p 80:80 -d -v $PWD/html://usr/share/nginx/html nginx 这个的意思是将主机目录映射到容器，我是将当前路径下(此时命令行所在路径)的html文件夹映射到虚拟机中/usr/share/nginx/html路径中，刚才截图中的第二个运行的另一个映射到80端口的东西就是他了，这样我们修改本地当前目录的html文件夹就相当于修改对应映射路径的文件夹了。 4. 使用Dockerfile构筑镜像&emsp;&emsp;这个其实也是部署静态文件的一种方法，不过我觉得比较具有代表性，就单独写一下。&emsp;&emsp;我们可以使用Dockerfile配置文件定制镜像，这个方法是我最推荐的，这里我用官方提供的nginx为模板来为我的Angular项目定制一个镜像。首先先用ng build -prod(react和vue的是啥我不知道)为项目生成dist文件夹，在dist文件夹下(因为项目在这里面，Dockerfile需要被导入文件在同一目录下)创建Dockerfile文件。 FROM nginx:1.15.9-alpineCOPY angular-routing-demo /usr/share/nginx/html/EXPOSE 80CMD [“nginx”, “-g”, “daemon off;”] 大致意思就是把nginx:1.15.9-alpine作为基础镜像，把文件扔到对应路径中，监听容器内的80端口，并且指定容器启动时的默认指令(这里设定的是Nginx的启动指令)。随后执行 docker build -t angular-demo . &emsp;&emsp;注意，指令最后面有一个”.”，我就是因为这个东西被坑了几个小时，执行完指令后，执行docker images指令就可以看到我们定制的镜像了。 随后使用刚才的创建并启动容器的指令，打开浏览器，访问localhost:8080(我把本地8080端口映射进容器的80端口了)，就能够直接访问到我的项目。&emsp;&emsp;Docker最大的好处就是任何环境下都能跑，不需要考虑环境，做到生产环境与开发环境一致，接下来就把我新建的容器移植到Windows下试试，执行 docker save -o angular-demo.tar angular-demo 这个指令是将指定镜像进行打包，如果要打包容器的话需要把save改成export。通用格式是 docker save -o dnsmasq.docker image_name -o指令是输出，dnsmasq.docker 是打包出来的文件，image_name是镜像名。上一条指令执行完之后会在执行指令的路径下生成angular-demo.tar文件，将这个文件拷贝到Windows下，其实完全可以上传到Docker Hub或者自己在服务器创建的本地仓库中，再用其他电脑拉下来，也省了用u盘了，不过重点不在这，不讨论太多。&emsp;&emsp;在Windows下使用命令行(貌似有图形化界面可以用，但是我喜欢命令行)，执行指令 docker load -i angular-demo.tar Windows上的Docker就会加载这个镜像，接着再使用上面的创建并启动命令，然后访问localhost:8080 完美运行，期间没有任何改动。 5. 最后&emsp;&emsp;本来只是为了Angular和SpringBoot移植用的，因为开发环境与生产环境肯定不是一样的，想起来我寒假的时候还在我的Mac里装了一个这玩意，而且还挺好用，于是决定入这个坑，往后要好好总结一下关于Docker的学习过程。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Django-rest-framework开发总结-项目搭建]]></title>
    <url>%2F2019%2F02%2F28%2FDjango-rest-framework%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93-%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Django-rest-framework开发总结-项目搭建写在前面&emsp;&emsp;我第一次开发web应用用的就是Django，甚至比最熟悉的Java还早，目前开发了两个应用，一个报名系统，一个考试系统，不敢说对这个框架很熟悉吧，但是也是有一些了解的，毕竟踩过不少坑，一个bug改三天的事也经常干，但是自从开了博客以来却没有总结过任何关于Django的文章，借着这次开发的机会总结一下，就当是做笔记了。这一回开发的并不是普通的Django应用，而是仅仅提供rest风格数据接口的服务接口的应用，这一过程中除了后台展示界面以外，我并没有参加关于任何前端的开发，目前来讲这也是一种国际上的主流用法，也是前后端分离的大趋势，有利于微服务的构建。 Django-rest-framework Django REST framework is a powerful and flexible toolkit for building Web APIs.Some reasons you might want to use REST framework: The Web browsable API is a huge usability win for your developers. Authentication policies including packages for OAuth1a and OAuth2. Serialization that supports both ORM and non-ORM data sources. Customizable all the way down - just use regular function-based views if you don’t need the more powerful features. Extensive documentation, and great community support. Used and trusted by internationally recognised companies including Mozilla, Red Hat, Heroku, and Eventbrite. &emsp;&emsp;这是Django-rest-framework官网给出的描述，大概意思就是这是一个构造webapi的应用，可以将数据序列化传输。虽然本身Django也有序列化的功能，但是并不怎么好用，接下来就来记录一下怎么搭建一个Django-rest-framework应用。&emsp;&emsp;我用的是macOS系统，工具用的是Pycharm，不过无所谓，反正python是一个跨平台语言，一切第三方库都是用Pycharm中的工具安装(其实还是pip，只不过是可视化界面安装)。 首先打开工具，选择Django工程，右边设置项目名称，选择环境，默认就是创建一个虚拟环境，这个一般不用动，这个滚动条往下拉，有一个Application name，这个是在使用Pycharm创建Django的时候可以直接创建一个app,这里面是填名字的，如果不填就不会创建，开始Django应用之后可以使用命令行创建，Template language是选择模板语言，因为我是创建webapi应用，所以这个不用管，然后点击create，开始下载Django，如果下载不成功可能是网速或者pip版本过低导致，换个网或者升级一下pip就行。&emsp;&emsp;首先在开始之前，我们要先干一件事情，因为pip默认是从外网服务器下载第三方包，所以一般比较慢，特别是用校园网有时会有波动导致无法连接，所以我们要先更改一下数据源，推荐使用清华大学或者豆瓣的数据源。直接点开左上角的Pycharm(windows上是file)，点Perference(windows是setting)，到下面这个界面。点开本项目的解释器(Interpreter)，可以看将当前Django在我们的环境之中，然后点下面的小加号，弹出的界面是搜索并安装第三方库的，先不管这个，点开下面的Manage Respositories，点里面的加号，添加https://pypi.tuna.tsinghua.edu.cn/simple/，添加清华的源，再下载的时候就是用清华的镜像源下载，速度比较快一些。然后在刚才那个搜索界面搜索并安装djangorestframework。然后再setting.py文件里向INSTALLED_APPS中添加&#39;rest_framework&#39;。&emsp;&emsp;虽然我对这个很熟悉，但是还是得捋一遍项目结构 &emsp;&emsp;Demo是项目文件，是这个工程的主干，__init__.py不用说了，所有python包的标识符，setting.py里面有各种配置，urls.py是路由文件，wsgi.py是跟服务器相关的，而下面的StudentApp是我创建的一个app，migration文件夹下会存放数据库的迁移文件，admin.py是Django默认提供了一套模板，用户可以将自己的模型用这个文件来注册，apps.py是对这个app的一些设置，model.py里面是模型类，可以直接迁移生成数据库。test.py是测试(我没用过)，views.py是视图，向外提供数据接口的。urls.py 和serializers.py是我自己建的，前一个是为了代码整洁建立的二级路由，后一个是将模型类数据序列化用的。 数据库配置&emsp;&emsp;在app文件夹中的__init__.py文件中添加12import pymysqlpymysql.install_as_MySQLdb() 这就相当于批量导入了，用来链接数据库。接着还要在setting.py中配置一下数据库，因为默认写的配置是sqllite3，但是我们需要mysql，所以要改一下。1234567891011DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'rest', 'USER':'root', 'PASSWORD':'******', 'HOST':'localhost', 'PORT':'3306', &#125;&#125; 在model.py中弄一个很简单的模型类1234567class Student(models.Model): name=models.CharField('姓名',max_length=100,default='no_name') sex=models.CharField('姓别',max_length=50,default='男') sid=models.CharField('学号',max_length=100,default='0') def __unicode__(self): return '%d: %s' % (self.pk,self.name) 嗯，不能再简单了，但是重点不在这里，在下面命令行里执行两条命令python manage.py makemigrations和python manage.py migrate，顺序不能反，前一个是在migrattions生成迁移文件，后一个是执行迁移,生成对应数据表。对了，当前django版本是2.17，不知道是哪个版本开始只支持mysql5.7以上的版本，5.7之前的版本生成的迁移文件没法执行。 序列化与反序列化&emsp;&emsp;然后开始序列化这一个模型类。1234567from rest_framework import serializersfrom .models import *class StudentSerializers(serializers.ModelSerializer): class Meta: model=Student#指定模型类 fields=('pk','name','sex','sid',) 创建一个StudentSerializers继承serializers.ModelSerializer，下面可以指定模型类，这样一配置，框架就会直接将数据库中的数据序列化为json格式传出，也可以接收json格式存入数据库，fields可以指定序列化哪一部分，如果向全部序列化的话直接等于&#39;__all__&#39;。 viewsets&emsp;&emsp;接着在views.py文件夹中写入1234567class StudentViewSet(viewsets.ModelViewSet): #指定结果集并排序 queryset = Student.objects.all().order_by('-pk') #指定序列化的类 serializer_class = StudentSerializers #任何用户可接入 authentication_classes = (AllowAny,) 只有这短短几行，但是现在已经完成了一整套的rest接口。关键的一点就是我这个StudentViewSet所继承的viewsets.ModelViewSet，这个类中包含了所有的接口实现，已经有了一套rest的api(不想使用他生成的接口，可以使用APIView自己写，还是原来的路由语法)，ModelViewSet中包含了：1234567891011121314151617def list(self, request): passdef create(self, request): passdef retrieve(self, request, pk=None): passdef update(self, request, pk=None): passdef partial_update(self, request, pk=None): passdef destroy(self, request, pk=None): pass 分别对应各种操作可以被重写，也可以添加，比如123456789def create(self, request, *args, **kwargs):#如果对面传过来的json串没有值的话，就是一个字符串，如果有值的话，会被作为json解析 '''提交数据验证并保存''' data = request.data anwser = NewTownRang.newtown(data) serializer = NewtownSerializer(data=anwser) if serializer.is_valid(raise_exception=True):#验证表单，如果错误会返回404 serializer.save() return Response(anwser,status=HTTP_200_OK) return Response(serializer.error_messages,status=HTTP_400_BAD_REQUEST) 这是重写了create方法，用来接收提交的数据，注释是我遇见的坑，如果对面传过来的是‘{“id”:1}’,这样的数据，我这边接收到的就是json格式，会解析为字典，如果对面传过来的是‘{“id”:}’，这个数据就是一个字符串。如果保存数据的话，第六行是必须要加的，因为数据要验证准确性，返回的数据也都是json格式，在python里就是字典。12345678@action(detail=True,methods=['get'])def getListById(self,request, *args, **kwargs): '''根据用户的编号(非学号，用户的pk，不是实验的pk)获取其所有信息''' pk = kwargs['pk'] serializer = NewtownSerializer(NewTown.objects.filter(user_id__exact=pk),many=True) data = serializer.data print(data) return Response(data,status=HTTP_200_OK) 这是增加一个方法，注意这个地方上面的注解原来是另外一个写法，网上大多数流传的也是那个方法，不过目前已经被官方定义为过时了，@action是最新的写法，里面有几个参数，detaill如果设置为true，那么生成url的时候会多一个/{id}/，这个是可以接收主键，methods是规定了这个api的请求方式，原来的主键是通过参数中有一个ip获取，不过目前是通过多参数列表获取pk，序列化的时候many=True必须要带，因为一般查询到的数据都是多条。 路由配置&emsp;&emsp;如果是用这个生成的话，就不需向之前一样写路由语法了，直接向app中自己建的urls.py文件夹中添加12345678from rest_framework import routersfrom StudentApp import viewsroute = routers.DefaultRouter()route.register(r'student',views.StudentViewSet)#url请求末端必须以反斜杠结尾urlpatterns = route.urls 这么做就相当于将StudentViewSet中所有的方法都放在了/student/路由下，接着在主文件中的urls.py添加1path('',include('Experimentation.urls')), 现在这个项目就能访问了，如果是在本机访问，只需要点右上角绿色的小箭头运行这个Django项目就行了，如果向通过ip访问的话，需要在setting.py中设置ALLOWED_HOSTS = [&#39;*&#39;]，这个是设置是为了限定请求中的host值，以防止黑客构造包来发送请求.只有在列表中的host才能访问。如果是开启DEBUG模式的时候默认的是本机url地址，就是127.0.0.1或者localhost，如果关闭DEBUG模式然后正式上线的话，这个值必须配置，在这里为了方便使用了通配符*，也就是所有host都可以访问。正式上线严禁使用通配符‘*’，而且必须关掉DEBUG模式(DEBUG = False)。大概就是设置成这样。123456ALLOWED_HOSTS = [ '.example.com', '.zzuli.com',]DEBUG = False 然后可以在命令行中运行python manage.py runserver 0.0.0.0:8000,这样就可以让其他主机使用ip访问数据接口。&emsp;&emsp;接下来用文档生成工具查看一下接口是否生成。&emsp;&emsp;可以看到一个非常标准的增删改查接口已经生成，先写这么多吧，坑还有很多，以后有时间慢慢写。]]></content>
  </entry>
  <entry>
    <title><![CDATA[哈希表]]></title>
    <url>%2F2019%2F02%2F18%2F%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[哈希表&emsp;&emsp;在学习Java和Python的时候经常使用的一种数据结构，像HashMap和字典都是封装好的哈希表，特点是他是一种动态储存结构，储存格式为key-value形式，支持INSERT、SEARCH、DELETE、UPDATE等一系列操作，他们的底层都是由各个程序语言的编译器维护出来了一个符号表，其中每个元素的key为任意字符串，与不同语言中现哈希表的实现的key对应。虽然看起来没什么，但是哈希表是实现字典操作的一个非常有效的数据结构，最坏情况下的搜索时间也与链表相同，也就是说他的最坏运行时间是O(n)，然而实际上在合理假设的情况下，哈希表的查找时间甚至达到了O(1)。 &emsp;&emsp;哈希表这种有key 的结构非常像数组中的下标，但是哈希表可以说是数组的推广。普通数组可以直接通过下标进行直接寻址，花费O(1)的时间就可以找到要查找值所在的位置。&emsp;&emsp;但是实际上很少存在这种一个萝卜一个坑的情况，关键字可能有时候并不能满足储存数，这个时候就需要哈希表来替代直接寻址表，哈希表使用后一个长度与实际储存的关键字数目成比例的数组来储存，哈希表中key与value并不是直接对应，而是根据key来计算下标，不过有些key可能会算出来同样的值，后面会说一个连接法来解决这个冲突问题，下面来仔细总结一下这一块内容。 1. 直接寻址表&emsp;&emsp;当key的范围比较小时，我们可以采取一个萝卜一个坑的策略，使用直接寻址法，简单粗暴。&emsp;&emsp;假设现在有一个动态集合，每个元素都是取自于全域U={0，1，2，3，…n}中的一个关键字，n不是很大，而且每个元素的关键字都不一样。我们可以很轻易的用一个数组来模拟一个直接寻址表，记为T[n+1]。其中每一个值对应的位置可以作一个槽，T就相当于全域U，下标为1的槽中就会插入key值为1的元素，下标为1的槽中就会插入key值为1的元素，依此类推。对应关系如图所示。&emsp;&emsp;基于直接寻址表的字典就很简单了，基本就类似于数组一样的操作，这里就贴出《算法导论》上的伪代码，不用C++重写了。12345678DIRECT-ADDRESS-SEARCH(T,k) return T[k]DIRECT-ADDRESS-INSERT(T,x) T[x.key]=xDIRECT-ADDRESS-INSERT(T,x) T[x.key]=NULL 这三种操作每一步耗费的时间都只有O(1)。&emsp;&emsp;有些直接寻址表并不像图中那样把数据放入外部对象中，再由表中指针指向该对象，而是直接把对象放进槽中，节省一部分空间。我们可以使用一个关键字表明这个槽是一个空槽，如果是数组这种结构我们就可以不需要关键字，直接使用下标来获取元素，但是需要用其他方法来确定某个槽为空。 2. 哈希表&emsp;&emsp;其实翻译过来叫散列表，但是说哈希表习惯了，就叫哈希表吧。&emsp;&emsp;直接寻址法有一个很大的缺点，如果你的全域U很大，甚至超过了电脑当前可用内存的范围，这时候划出一个大小为U的数组T不太现实。有时候可能全域U的范围很大，但是实际储存的值却不多，那剩下的都是空值，那这样又会造成内存的极大浪费。&emsp;&emsp;当字典中所有的关键字K与所有可能的关键字全域U小很多时，哈希表所需要的储存空间比直接寻址表要少许多，相应的我们的储存需求也降低到了 $\Theta$(|K|)，但是查找一个元素的时间依旧是只有O(1)，但是这是相对于平均情况来讲，而对于直接寻址法来说，这只是最坏时间，我们在这里相当于用时间来换取空间了。&emsp;&emsp;在直接寻址表中，我们将关键字为k的值放在槽k中，而在哈希表中，我们将值放在槽h(k)中，这里我们使用了哈希函数h来对k值进行计算求得槽的位置，通过这种方式，h可以讲全域中U中所有的值通过计算映射到哈希表T中的槽位上,h(k)就是k的哈希值，哈希函数缩小了原本数组所需的下标范围，把|U|的大小缩减到了实际关键字数量K的大小。如图所示。 2.1 链接法&emsp;&emsp;虽说缩小了全域的范围，但是哈希函数并不能保证每个k值经过计算后都不相同，很有可能如上图所示，k2与k5经过哈希函数计算后被映射到了同一个槽中，一种解决办法就是让哈希函数尽可能的“随机”，但是无论哈希函数怎么“随机”，全域U的大小比数组T的大小大，至少有两个或者多个关键字的哈希值相同，想要避免冲突是不可能的，一方面是使用更好的哈希函数使哈希值尽量随机，另外还有几种方法，先说链接法。&emsp;&emsp;在链接法中，每一个槽中都会有一个指针，作为一个链表的表头，当遇见相同的哈希值时，相同的哈希值都会被放在链表之中，如果该槽中没有元素，这个指针指向null。使用链接法后，哈希表T的字典操作就是另一种写法(还是用伪代码)。12345678CHAINED-HASH-INSERT(T,x) insert x at the head of list T[h(x.key)]CHAINED-HASH-SEARCH(T,K) search for an element with key in list T[h(k)]CHAINED-HASH-DELETE(T,x) delete x from the list T[h(x.key)] 插入的最坏运行时间依旧是O(1)，而且可能要快一些，如果是查找的话，最坏运行时间与表长度成正比，假设这是一个双向链表，删除一个元素的运行时间是O(1)。但是如果是单向链表情况下，删除与查找的渐进运行时间是一样的。 2.2 链接法分析&emsp;&emsp;给定一个能存放n个元素，具有m个槽位的哈希表T，定义T的装载因子$\alpha$为n/m，就是一个链的每个槽位的平均装载数，我们对链接法的分析借助$\alpha$来说明。&emsp;&emsp;首先分析最坏性能，即所有的值都被映射到了一个槽中，那么当前哈希表中只存在一张长度为n的链表，这时候最坏的查找时间为$\Theta$(n)，再加上哈希函数的运算时间，显然，这种情况跟用链表没啥区别，性能比较差。&emsp;&emsp;如果想要尽量避免最坏性能，而且在平均性能附近浮动的话，那么很大部分上就取决于哈希函数h，一个好的哈希函数可以尽可能的将关键字向m个槽中分散。如果一个元素等可能的散列到m个槽中的任意一个，，与其他元素无关，这样可以称为简单均匀散列，期望值为$\alpha$。&emsp;&emsp;假设O(1)时间内计算出哈希值h(k),而查找关键字元素k的时间线性的依赖于表T[h(k)]的长度nh(k)，先不考虑那个计算哈希值的时间，先来看一下查找的期望值，一共就两种情况，要么找到，要么找到不到。 定理一： 在简单均匀散列的假设下，对于用链接法解决冲突的散列表，一次不成功查找的平均时间为$\Theta$(1+$\alpha$) &emsp;&emsp;任何尚未被存储在表中的关键字k都等可能地被散列到m个槽中，然而在查找k的时候，不成功的话去，肯定是查找到了h(k)所在链表T[h(k)]的末尾，期望时间也是如此此时期望长度为E[nh(k)]=$\alpha$(平均情况下每个槽中有$\alpha$个数据，查找不成功时，这$\alpha$个数据肯定都被遍历过)。再带上计算哈希值用的$\Theta$(1)时间，一共用了$\Theta$(1+$\alpha$)。 定理二： 在简单均匀散列的假设下，对于用链接法解决冲突的散列表，一次成功查找所需的平均时间是$\Theta$(1+$\alpha$)。 引理5.1 给定一个样本空间S和S中的的一个事件A，设 XA=I{A}，那么E[XA]=Pr{A}。 这是在《算法导论》第五章概率分析和随机算法中第5.2节指示器随机变量中的一个引理，I{A}是指示器随机变量，它为概率与期望之间的转换提供了一个便利的方法，具体不在此赘述，这不是重点，这是概率论的范畴，在这里引入只是下面证明需要使用这个定理。 &emsp;&emsp;查找成功的情况有点复杂，因为每个链表被被查找到的情况并不是等可能的，某一条链表被查找到的概率与所包含的元素数成正比(因为出现在每一个地方的情况是等可能的，链表越长，概率越大)，但是查找的期望时间并没有改变。&emsp;&emsp;假设要查找的元素存在于表中，且出现在等可能位置，在对元素x的一次成功的查找中，所检查的元素数比x所在的链表中，出现在x前面的元素数多1。在该链表中，出现在x之前的元素都是在x之后插入的，这是因为新的元素都是在表头插入的。为了确定所检查元素的期望数目，对x所在的链表，在x之后插入到表中的期望元素数加一，再对表中n个元素x取平均，设xi 表示插入到表中的第i 个元素，i=1,2,…,n，并设ki=xi.key。对关键字ki和kj，定义指示器随机变量Xij=I{h(ki)=h(kj)}。在简单一致散列的假设下，有Pr{h(ki)=h(kj)}=1/m，从而根据引理(5.1)，有E[Xij]=1/m。于是，在一次成功的查找中，所检查元素的期望数目为&emsp;&emsp;先说一下这一个公式是怎么来的，首先在离散数学里面，计算一个算法在平均状态下的计算复杂性，可以转变成一个变量的期望值，设一个实验的样本空间是可能输入aj(j=1,2,…,n)的集合，且令随机变量X对aj 赋值是aj 作为输入时该算法用到的操作次数。基于我们对输入的了解，对每个可能的输入aj 赋给一个概率p(aj)。那么该算法在平均状态下的复杂性是每个可能的输入的概率为1/n。也就是说x等于表中第i 个元素的概率是1/n,指示器随机变量Xij=I{h(ki)=h(kj)}的含义是如果i 和j 散列在同一个槽位当中，也就是在同一个链表中时，加1次。因此，综上所述，在x之后插入到表中的所检查元素的总数目为：算法在平均状态下的所检查元素的总数目为：&emsp;&emsp;因此，一次成功查找的全部时间(包含哈希函数运算时间$\Theta$(1))为$\Theta$(1+$\alpha$)。&emsp;&emsp;至此我们已经将链接法分析完毕，我们来看一下分析结果，如果哈希表中槽数与表中的元素成正比，则有n=Q(m)，从而a=n/m=O(m)/m=O(1)，所以我们查找需要的是平均常数时间，当链表使用双向链表时，插入操作最坏当情况下需要O(1)时间，删除最坏情况下需要的也是O(1)时间，所以，全部的字典操作都可以在O(1)时间内完成。 3. 哈希函数&emsp;&emsp;这一块应该是在哈希表之下，但是东西太多，只好单独拉出来讲。&emsp;&emsp;哈希表中保证元素能够等可能的映射在每一个槽中，一个好的哈希函数是非常关键的。一个好的哈希函数应该满足简单均匀散列假设：每个关键字都被等可能的散列到m个槽位中的任意一个，并与其他关键字散列到哪个槽位无关。然而实际情况下并没有办法检查这一条是否成立，因为我们一般都不知道关键字散列的具体分布。一般来说都是将关键字转化为自然数，比如ASCII码，然后再实用哈希函数进行处理。下面总结几个常用的计算哈希值方法。 3.1 除法散列法&emsp;&emsp;设k为关键字，m为槽数。计算哈希函数是我们可以用k对m取余数，将关键字k映射到m中对某一个槽中： h(k) = k mod m 比如m = 12，k = 100，那么h(k) = 4,由于只有一次运算，所以速度上还是可以的。&emsp;&emsp;书上说在使用除法散列法时，需要避免一些m的选值，比如m不能取2的幂，因为如果m = 2p，则h(k)就是k的最低p位数字，一开始我根本就不懂这一句话是什意思，后来看了一下英文原版发现如果m = 2p，h(k)的值会等于k的二进制的最低的p位，如果m = 8，就像下面这张从网上找到的图一样。仔细分析一下，如果一个数越是接近2i,其二进制中就会存在大段连续的0和1，取余运算的本质无非是个减去m的n倍的减法。那么做减法的时候，m中间大段的0或1就会让哈希原值的中间一段有非常大的可能性仍然保持原样。哈希函数的本质目的就是混淆，原值的变化产生哈希值无规律、等概率、不可预测、不能逆推的变化为最佳。如果做完哈希运算之后，哈希值和原值中间居然有一大段二进制位保持不变，那么这个哈希函数就可以说是失败到不能再失败了。就着这个哈希表而言，取这样的值在概率上也会让哈希值重复度过高，不符合均匀散列，而m取素数的时候相对来说能让哈希值分布得更均匀一些。所以在设计哈希函数的时候，最好要考虑关键字的所有位。 3.2 乘法散列法&emsp;&emsp;执行一个乘法散列法分两步，第一步让关键字k乘一个常数A(0&lt;A&lt;1)，，提取其结果的小数部分，然后用m乘以这个值，再向下取整。 h(k) = ⌊m(kA MOD 1)⌋ 与除法散列法相比，乘法散列法中m的选择并不关键，一般选为2的幂，比较推荐A的值是 $\dfrac{\sqrt{5}-1}{2}$ = 0.618 033 9887…唯一的缺点就是他生成的哈希函数没有除法散列法那么均匀。 3.3 全域哈希&emsp;&emsp;无论什么样的哈希算法，如果某个恶意的用户针对该哈希算法选择性的输入关键字，那么就会将所有的关键字全部散列到同一个槽中，使得哈希表运行效率变成最低，不论多好的哈希函数都会这样，唯一的方法就是每一次计算使用的都不是一个哈希函数，而是随机的，这种方法称为全域哈希，不管输入什么样的关键字，性能都很好。 4. 开放寻址法&emsp;&emsp;开放寻址法中，所有的元素都被存放在哈希表中，也就是表中的每一项要么是空，要么是一个元素。当要查找的时候，就要检查整张表，直到找或者遍历了整张表。这里没有链表，数据也不会在表外，所以在开放寻址法中，哈希表可能会被填满，以至于无法插入新元素，装载因子$\alpha$绝对不会超过1。&emsp;&emsp;开放寻址法中不需要指针或者对象，只需要计算出槽的存取序列，可以节省原来指针空间以提供更多的槽，减少了冲突，提高了速度。&emsp;&emsp;开放寻址法中如果要插入元素，需要检查哈希表，这个过程叫探查，直到找到一个空的位置插入，检查不一是从头开始按顺序来(这样的执行效率是$\Theta$(n))，而是依赖于待插入的关键字，为了确定探查槽，现在需要加一个参数，对每一个关键字k，都会有一个探查序列，探查序列可能是顺序的，也可能基于某种方法，是0到m-1的一个排列， &lt;h(k,0),h(k,1),…,h(k,m-1)&gt; 如果给定一个关键字k，首先会看h(k,0)是否为空，如果为空，则插入；如果不为空，则看h(k,1)是否为空，以此类推，但是删除的时候比较麻烦，一般必须删除关键字的应用中，链接法更常见。其中具体的操作在后面实现哈希表的时候再说。&emsp;&emsp;有三种方法可以计算开放寻址法中的探查序列，但是都不能满足均匀散列的假设，，因为他们产生的不同的探查序列都不会超过m2个(均匀散列要有m!个)。 4.1 线性探查&emsp;&emsp;h’为辅助散列函数： h(k,i) = (h’(k) + i) mod m i = 0,1,…,m-1 &emsp;&emsp;给定一个关键字k，首先探查槽位T[h’(k)]，然后是T[h’(k) + 1]，直到T[m-1]，再从T[0]开始，直到最后的T[h’(k)-1]，相当于是从初始位置顺序开始探查。在线性探查中，初始探查位置决定了整个序列，所以有m种不同的探查序列。&emsp;&emsp;线性探查有个缺点，就是一次群集。当表中i，i+1，i+2位置上都已经填满时，下一个哈希地址为i，i+1，i+2，i+3的关键字记录都将竞争i+3的位置。随着连续被占用的槽位不断增加，平均查找时间也不断增加。 4.2 二次探查&emsp;&emsp;二次探查的函数： h(k,i) = (h’(k) + c1i +c2i2) mod m i = 0,1,…,m-1 c1c2为辅助常数，初始探查位置为T[h’(k)]，后续的探查位置要在此基础上加上一个偏移量，该偏移量以二次的方式依赖于探查号i。这种探查的效果要比线性探查好。但是，如果两个关键字的初始探查位置相同，那它们的探查序列也是相同的，因为h(k1,0)=h(k2,0)蕴含着h(k1,i)=h(k2,i)，这一性质会导致二次群集。类似于线性探查，二次探查也仅有m个不同的探查序列。 4.3 双重散列&emsp;&emsp;双重散列的函数： h(k,i) = (h1(k) +i*h2(k)) mod m &emsp;&emsp;i = 0,1,…,m-1 &emsp;&emsp;双重散列中有两个辅助函数h1和h2，他的初始位置是T[h1(k)],后续的探查位置在此基础上的偏移量为h2(k)对m取余数。&emsp;&emsp;双重散列比线性探查和二次探查都要好，与前两者不同，双重探查的的探查序列以两种不同的方式依赖于关键字k。为能查找整张表，值h2(k)要与表的大小m互质。确保这个条件成立的一种方法是取m为2的幕，并设计一个总产生奇数的h2。另一种方法是取m为质数，并设计一个总是产生较m小的正整数的h2。因为当m取质数或者取2的幂时，双重散列法会有$\Theta$(m2)种探查序列，比前两个只有$\Theta$(m)种探查序列更接近均匀假设。 &emsp;&emsp;因为证明过程较长，就不再对开放寻址法进行分析了，具体过程在《算法导论》中有详细推导过程。&emsp;&emsp;在网上搜寻有关资料的时候看到了一个比较能清晰解释但是有点恶心的说法，贴上原文链接和文章内容。原文链接：https://blog.csdn.net/shaobingj126/article/details/8156675 &emsp;&emsp;简单地讲，也就是说，一间厕所，来了一个顾客就蹲其对应的位置，如果又来一个顾客，把厕所单间门拉开，一看里面有位童鞋正在用劲，那么怎么办？很自然的，拉另一个单间的门，看看有人不，有的话就继续找坑。当然了，一般来说，这个顾客不会按顺序一个一个地拉厕所门，而是会去拉他认为有可能没有被占用的单间的门，这可以通过闻味道，听声音来辨别，这就是寻址查找算法。如果找遍了所有厕所单间，看尽了所有人的光屁股，还是找不到坑，那么这座厕所就该扩容了。当然了，厕所扩容不会就只单单增加一个坑位，而是综合考虑成本和保证不让太多顾客拉到裤子里，会多增加几个坑位，比如增加现有坑位的0.72倍。为什么是0.72呢，这是所长多年经营所得到的经验值，为了让自己的经验发扬光大，需要出去演讲，又不能太俗，总不能说“厕所坑位因子”吧，那就把把0.72叫做“装填因子”或者“扩容因子”吧。目前很多产品使用0.72这个常数。 5. 完全散列&emsp;&emsp;哈希技术的平均性能非常优异，而且当关键字集合为静态时，还能提供非常棒的最坏性能。所谓静态就是将关键字集合存入表中，不再变化，例如程序语言中的保留字集合就是一个静态集合。有一种哈希方法叫完全散列，使用该方法查找时，最坏的性能也是O(1)。&emsp;&emsp;该方法采用二级散列，而且每一级都是全域哈希，如图所示。&emsp;&emsp;第一级就是一个带链接和哈希表，利用从全域散列函数集合中的一个函数h，将n个关键字放入m个槽中。然后再用一个比较小的二次哈希表Sj和哈希函数hj将其再次映射到二次哈希表，而不是使用连接法建立关键字链表。利用好的哈希函数hj，可以使其再第二级不出现冲突。&emsp;&emsp;为了让第二级不产生冲突，需要让哈希表Sj的大小mj为j中关键字nj的平方。虽然说总体看起来储存需求很大，但是可以适当选择第一级的哈希函数，使预期储存空间限制在O(n)。大致就是如此，还有一些细节就不再阐述了。 &emsp;&emsp;因为有其他事情的原因，这个总结断断续续写了好几天，说实话，还有很多细节没有写出来，但是我觉得说得太细了反而会影响整体。这不算很难的一个数据结构，可以说比较简单，而实现起来也容易，但是分析起来却超过了大多数的数据结构。先消化一段时间，先准备计赛，然后开始树和图的总结。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb安装]]></title>
    <url>%2F2019%2F02%2F05%2Fmongodb%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[mongodb安装&emsp;&emsp;大年初一无聊，玩玩mongodb。 写在前面&emsp;&emsp;再知乎上经常听到大名鼎鼎的nosql类型数据库，相对于关系型数据库来说，这个东西部署很简单，储存方式为虚拟内存+持久化存储，总结起来一个字————快。nosql存起来是key-value形式的，也支持集合和对象但是并不支持事务，也不能连表查询。&emsp;&emsp;传统的数据库体系非常成熟，可以进行事务操作，能用SQL语句进行复杂操作，不过当数据量极大时，关系型数据库效率会大打折扣，我猜一个很大的原因是外键的关系，查询中一系列组合索引会消耗很长时间，好多公司会针对这种情况进行分库分表，阿里巴巴开发手册里也说明尽量不要使用外键，将所有的关联事务在服务层解决。But，这种情况我从来没遇见过，我这种小菜鸡目前写的东西是肯定没有这么多用户的，这些都是我听说的。&emsp;&emsp;而内存级数据库就厉害了，利用内存的io性能可以进行极速读写和查询，虽然不能有SQL进行复杂的查询，但是mongodb能写JavaScript啊，总比那辣鸡SQL舒服多了，除了没有事务之外，其他基本就不是事了。废话不说，下面讲安装。 安装mongodb OS: MacOS 安装homebrew&emsp;&emsp;MacOS包管理工具，可以使用brew命令安装工具，和Ubuntu下的apt-get命令十分类似，不过MacOS中并没有自带homebrew，需要自己进行下载。 /usr/bin/ruby -e “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 很简单，直接输入这一条指令就行了，不过必须要有Xcode才能运行成功，下载时如果发现没有Xcode会提示安装，期间需要输入密码。 使用homebrew安装mongodb&emsp;&emsp;其实还有其他的方法可以安装mongodb，比如直接从官网下载，不过需要自己配置环境变量和写配置文件，然而我比较懒，使用homebrew安装的mongodb是自动配置好环境变量的，所以就使用这种方法了。 brew install mongodb 简单粗暴，一个命令就装完了，不过需要点配置，先在根目录(就是跟主机名一样的那个目录)下，创建/data/db文件夹，现在直接输入一个mongod指令是可以直接打开数据库的，不过会报Warning，因为mongodb是可以不需要用户就可以进入的，但是会对你进行警告，所以现在要添加一个账户。先进入数据库。输入指令,出现>就代表进入了数据库指令界面。 >use admin 切换到admin。 >db.createUser( { user: “admin”, //用户名 pwd: “passwd”, //密码 roles: [ { role: “userAdminAnyDatabase”, db: “admin” } ] //设置权限 }) 添加用户，权限为userAdminAnyDatabase，可以管理所有用户。然后可以执行命令 mongod –auth –port 27017 –dbpath ~/data/db --auth表示需要用户验证，--port表示端口号，默认就是27017，--dbpath表示数据库存储位置。 再开启一个命令行界面，输入指令mongo，在之前的命令行界面可以看到。 2019-02-05T22:32:28.130+0800 I NETWORK [listener] connection accepted from 127.0.0.1:49902 #1 (1 connection now open) 说明接入了一个服务。接着输入指令。 > use adminswitched to db admin> db.auth(‘root’,’root’);1 切换到admin之后用db.auth(“用户名”,”密码”);就可以进入用户了，1表示成功，主命令行会出现如下内容，表示认证成功。 2019-02-05T22:35:38.116+0800 I ACCESS [conn1] Successfully authenticated as principal admin on admin 这就算打开了，不过还要知道这个数据库怎么关，根据网上的资料来说，不能直接强制关闭，需要在界面中执行db.shutdownServer();命令，于是我愉快的执行了这个指令，结果…… assert failed : unexpected error: Error: shutdownServer failed: not authorized on admin to execute command { shutdown: 1.0 }Error: assert failed : unexpected error: Error: shutdownServer failed: not authorized on admin to execute command { shutdown: 1.0 } at Error () at doassert (src/mongo/shell/assert.js:11:14) at assert (src/mongo/shell/assert.js:20:5) at DB.shutdownServer (src/mongo/shell/db.js:212:9) at (shell):1:4 emmmm……他给我抛了个异常，费了劲跑到了官网查看一下官方文档，发现网上那些教程净瞎写，一篇破教程被十几个人转发还没人发现不对，也是服气。原来在mongodb并不像mysql一样有一个可以拥有一切权限的admin用户，而且可以看一下mongodb也根本没有一个admin库，所谓userAdminAnyDatabase权限也就是可以管理所有用户，并没有权限关闭数据库，所以我们要给这个用户添加可以shutdown角色hostManger。 > db.grantRolesToUser( “admin” , [ { role: “hostManager”, db: “admin” } ]) 现在进入admin登录用户后运行db.shutdownServer(); server should be down…2019-02-05T22:50:38.485+0800 I NETWORK [js] trying reconnect to 127.0.0.1:27017 failed2019-02-05T22:50:38.487+0800 I NETWORK [js] reconnect 127.0.0.1:27017 failed failed 这就已经关闭了，同时也可以看见运行mongodb的那个命令行进程也关闭了。&emsp;&emsp;还有要运行下面这一段命令，添加执行命令权限，不然什么都干不了。 db.grantRolesToUser ( “admin”, [ { role: “__system”, db: “admin” } ] ) &emsp;&emsp;具体怎么用再说吧，等后面学习总结。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>nosql</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本数据结构]]></title>
    <url>%2F2019%2F02%2F02%2F%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[基本数据结构&emsp;&emsp;虽然以前都学过也都用过，但是并没有具体总结过，这里权当是复习了，使用C++的面对对象方法重新实现一下这几个数据结构。 1. 栈&emsp;&emsp;一种动态集合，增加和删除操作元素的位置都是预定好的，在栈中，最先被删除的元素就是最后被放入的元素，实行的是后进先出，我们把数据放入栈称作压入，把数据拿出栈叫做弹出，一般把栈比做把网球放进球筒中，拿出来的时候肯定先拿出最后放进去那个，打个不恰当的比方就是一个人吃了再吐,先上代码。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include&lt;iostream&gt;using namespace std;template&lt;class T&gt;class node&#123; public: T value; //储存的值 node&lt;T&gt;* next; node() :next(nullptr)&#123;&#125; //构造函数 node(T t) :value(t), next(nullptr)&#123;&#125;&#125;;template&lt;class T&gt;class myStack&#123; int cnts; //入栈数量 node&lt;T&gt; *head; //栈的头部public: myStack()&#123; cnts = 0; head = new node&lt;T&gt;; &#125; void stackPush(T arg); //入栈 T stackPop(); //出栈 T stackTop(); //获取栈顶元素 void printStack(); //打印栈 int counts(); //获取栈内元素个数 bool isEmpty(); //判断空&#125;;template&lt;class T&gt;void myStack&lt;T&gt;::stackPush(T arg)&#123; node&lt;T&gt; *pnode = new node&lt;T&gt;(arg); //申请入栈元素的空间 pnode-&gt;next = head-&gt;next; head-&gt;next = pnode; cnts++;&#125;template&lt;class T&gt;T myStack&lt;T&gt;::stackPop()&#123; if (head-&gt;next!=nullptr) &#123; node&lt;T&gt;* temp = head-&gt;next; head-&gt;next = head-&gt;next-&gt;next; T popVal = temp-&gt;value; delete temp; cnts--; return popVal; &#125;&#125;template&lt;class T&gt;T myStack&lt;T&gt;::stackTop()&#123; if (head-&gt;next!=nullptr) &#123; return head-&gt;next-&gt;value; &#125;&#125;template&lt;class T&gt;void myStack&lt;T&gt;::printStack()&#123; if (head-&gt;next != nullptr) &#123; node&lt;T&gt;* temp = head; while (temp-&gt;next != nullptr) &#123; temp = temp-&gt;next; cout &lt;&lt; temp-&gt;value &lt;&lt; endl; &#125; &#125;&#125;template&lt;class T&gt;int myStack&lt;T&gt;::counts()&#123; return cnts;&#125;template&lt;class T&gt;bool myStack&lt;T&gt;::isEmpty()&#123; if (cnts) return false; else return true;&#125; &emsp;&emsp;这回就直接写一个C++的模版类stack.hpp，扔到头文件里是为了更好的调用。我觉得一个类能更好的描述一个操作集合，里面的数据用了链表结构储存，关于链表下面再说。&emsp;&emsp;当这个对象被实例化的时候，这个栈是空的，此时cnts=0，如果对一个空栈进行弹出操作，那么通常会抛出一个异常，这叫做栈下溢，如果添加数据超过存储上限，称为栈上溢，不过书上这么说是因为书上是用数组实现的，而链表长度不定，最大长度理论上和内存条大小有关，所以不考虑栈上溢，而且链表增加与删除只需要修改节点就行了，非常容易实现，栈的插入、删除、判空的操作时间均为O(1)。 2. 队列&emsp;&emsp;队列的插入操作叫做入队，删除操作叫做出队，队列与栈的操作相反，是先进先出，队列有队头与队尾，与我们排队一样，先到的人先办事，废话不说，上代码。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;iostream&gt;using namespace std;template&lt;class T&gt;class node&#123; public: T value; //储存的值 node&lt;T&gt;* next; node() :next(nullptr)&#123;&#125; //构造函数 node(T t) :value(t), next(nullptr)&#123;&#125;&#125;;template&lt;class T&gt;class myQueue&#123; public: int cnts; //入队数量 node&lt;T&gt; *head; //队的头部 node&lt;T&gt; *tail; //队的尾部 myQueue()&#123; cnts = 0; head = new node&lt;T&gt;; tail = new node&lt;T&gt;; tail-&gt;next=head; &#125; void enqueue(T arg); //入队 T dequeue(); //出队 T getfrist(); //获取队首元素 void printQueue(); //打印队列 int counts(); //获取队列内元素个数 bool isEmpty(); //判断空&#125;;template&lt;class T&gt;void myQueue&lt;T&gt;::enqueue(T arg)&#123; node&lt;T&gt; *pnode = new node&lt;T&gt;(arg); //申请入队元素的空间 if(cnts==0)&#123; tail-&gt;next=pnode; head-&gt;next=pnode; &#125;else&#123; tail-&gt;next-&gt;next=pnode; tail-&gt;next=pnode; &#125; cnts++;&#125;template&lt;class T&gt;T myQueue&lt;T&gt;::dequeue()&#123; if(cnts!=0)&#123; node&lt;T&gt;* temp = head-&gt;next; head-&gt;next = head-&gt;next-&gt;next; T popVal = temp-&gt;value; delete temp; cnts--; return popVal; &#125; &#125;template&lt;class T&gt;T myQueue&lt;T&gt;::getfrist()&#123; if(cnts!=0)&#123; return head-&gt;next-&gt;value; &#125;&#125;template&lt;class T&gt;void myQueue&lt;T&gt;::printQueue()&#123; if (head-&gt;next != nullptr) &#123; node&lt;T&gt;* temp = head; while (temp-&gt;next != nullptr) &#123; temp = temp-&gt;next; cout &lt;&lt; temp-&gt;value &lt;&lt; endl; &#125; &#125;&#125;template&lt;class T&gt;int myQueue&lt;T&gt;::counts()&#123; return cnts;&#125;template&lt;class T&gt;bool myQueue&lt;T&gt;::isEmpty()&#123; if (cnts) return false; else return true;&#125; &emsp;&emsp;同样的，队列队删除、插入、判空操作需要的时间也都是O(1)。 3. 链表&emsp;&emsp;链表中的各个对象按线性顺序排列。与数组不同，链表的存储并不是一整块内存区域，也就是说他顺序并不是由下标来决定的，而是由每一个元素中的指针来决定的，在内存中的表现就是他存储的数据是离散的，而且长度可以不确定，大小随着长度增长而增长，其长度理论上可以占满整个内存，在实现队列和栈时并没有对容量上限作出检查的原因就是如此。&emsp;&emsp;链表中的每一个节点就是一个对象，每一个对象都有一个key和一个指针，有时还会包含一些卫星数据，如果x为链表中的一个元素，那么x的指针x.next则指向他的下一个后继元素，一般链表都会有一个头节点，称为头，这个节点相当于一个“柄”，他的后面连接着整条链表，我们通常通过这个“柄“对这个链表操作，最后一个节点被称为尾，头节点的指针为空，说明该链表为空。&emsp;&emsp;链表不止一种形式，上面说的是单链表，还有一种双向链表，在单链表的基础上，每一个节点增加了一个前驱指针，可以从链表的头和尾同时进行访问，也就是能够进行顺序和逆序操作。此外还有循环链表，这种链表的尾指针指向头指针，直观看来他的结构是闭合的，是一个各个元素组成的圆环，下面讨论的都是乱序的双向链表,而且有头节点。&emsp;&emsp;先看一下关于链表的实现。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#include&lt;iostream&gt;using namespace std;template&lt;class T&gt;class node&#123; public: T value; //储存的值 node&lt;T&gt;* next;//后继指针 node&lt;T&gt;* pre;//前驱指针 node() :next(nullptr)&#123;&#125; //构造函数 node(T t) :value(t), next(nullptr)&#123;&#125;&#125;;template&lt;class T&gt;class Link&#123; private: int count; node&lt;T&gt; *head;//头节点 public: Link()&#123; count=0; head = new node&lt;T&gt;; &#125; void addnode(T t);//增加节点 void deletenode(T t);//删除节点 node&lt;T&gt;* searchnode(T t);//查找节点 void printlink();//打印链表&#125;;template&lt;class T&gt;void Link&lt;T&gt;::addnode(T t)&#123; node&lt;T&gt; *p=new node&lt;T&gt;(t); if(count!=0)&#123; p-&gt;next=head-&gt;next; head-&gt;next-&gt;pre=p; head-&gt;next=p; p-&gt;pre=head; &#125;else&#123; p-&gt;next=head-&gt;next; head-&gt;next=p; p-&gt;pre=head; &#125; count++;&#125;template&lt;class T&gt;node&lt;T&gt;* Link&lt;T&gt;::searchnode(T t)&#123; if(head-&gt;next!=nullptr)&#123; node&lt;T&gt; *temp; temp=head; while(temp-&gt;next!=nullptr)&#123; temp = temp-&gt;next; if (temp-&gt;value==t) &#123; return temp; &#125; &#125; cout&lt;&lt;"未找到该元素"&lt;&lt;t&lt;&lt;"！"&lt;&lt;endl; return nullptr; &#125; return nullptr;&#125;template&lt;class T&gt;void Link&lt;T&gt;::deletenode(T t)&#123; node&lt;T&gt; *temp; temp=searchnode(t); if (temp!=nullptr) &#123; if(temp-&gt;pre!=nullptr)&#123; temp-&gt;pre-&gt;next=temp-&gt;next; &#125; if(temp-&gt;next!=nullptr)&#123; temp-&gt;next-&gt;pre=temp-&gt;pre; &#125; &#125; count--;&#125;template&lt;class T&gt;void Link&lt;T&gt;::printlink()&#123; if (head-&gt;next != nullptr)&#123; node&lt;T&gt;* temp = head; while (temp-&gt;next != nullptr)&#123; temp = temp-&gt;next; cout &lt;&lt; temp-&gt;value &lt;&lt; " "; &#125; cout&lt;&lt;endl; &#125;&#125; &emsp;&emsp;searchnode使用简单的线性方法去遍历链表并返回元素指针，如果不存在key值为t的对象，则返回nullptr，在搜索有n个对象的链表nullptr的最坏运行时间为$\Theta$(n)，也就是遍历整条链表。&emsp;&emsp;addnode可以像链表中插入关键字为t的元素，将其链接在链表的前端，其运行时间为O(1)。&emsp;&emsp;deletenode将一个元素x从链表中移除，需要先通过searchnode找到该对象的指针，通过修改其前一个元素和后一个元素的前驱指针与后继指针即可将其移出链表，操作时间为O(1)，但是删除操作首先调用了searchnode方法，所以又耗费了$\Theta$(n)时间，所以其最坏情况为$\Theta$(n)。&emsp;&emsp;printlink使用线性方法遍历整个链表，并逐一打印其元素的值，由于其必定遍历整条链表，所以对于n个对象的链表来说，他的运行时间为$\Theta$(n)。&emsp;&emsp;《算法导论》中还介绍了一种叫做哨兵的哑对象，代表NIL，属性与其他对象相同，它可以将常规的双向链表转化为有哨兵的双向循环链表，它位于表头与表尾之间，next指向表头，pre指向表尾，类似的，表尾的next和表头的pre都指向哨兵，哨兵的next指向表头，我们可以去掉head，把他的引用代替为对哨兵.next的引用，空链表只有一个哨兵，他的next和pre都指向他自己。哨兵并不能降低该数据结构操作的时间渐进界，但是可以降低一个常数因子，可以做到代码简洁，但是并不能提高速度，如果有多条短链表的话，就不建议使用哨兵，因为会加大内存开销。 4. 有根树&emsp;&emsp;链表可以推广到任意的同构数据结构，比如对栈和队列，我就是使用链表对书上用数组描述的伪代码进行重构。树与链表很类似，都是使用节点进行数据存储，但是链表是一条长链，而树是分叉的，指向其他节点的指针并非一个，会随树的种类而变化。 4.1 二叉树&emsp;&emsp;顾名思义，这个树有两个叉，二叉树的每一个节点存在着一个属性和三个节点：p、left、right，分别代表父节点、左子节点和右子节点，如果p节点为空，则说明该节点为根节点，如果p根节点为空，则说明该二叉树为空，如果二叉树为近似完全二叉树(若设二叉树的深度为k，除第k层外，其他各层（1～（k-1）层）的节点数都达到最大值，且第k层所有的节点都连续集中在最左边)，可以将其称之为堆(不是Java的垃圾堆)，后面讲到堆排序的会具体说明。二叉树的代码等到对二叉搜索树进行总结时再贴出。 4.2 分支无限制有根树&emsp;&emsp;这个货就厉害了，他的分叉没有限制，但是麻烦的是因为他们没有限制，所以不知道他下面到底连了几个节点，而且如果设置一个很大的常数k作为子节点数量，但是只有几个子节点有数据，那么内存空间就大大的被浪费了。&emsp;&emsp;不过还是有方法解决这个问题，而且对于任意n个子节点的有根树来说，只需要O(n)的储存空间。这种方法叫做左孩子右兄弟表示法，与二叉树类似，有一个指向父节点的p指针，但是指向子节点的指针并不是n个，而是只有两个指针left-child和right-sibling。&emsp;&emsp;left-child指向该节点众多子节点中最左侧的那一个节点，right-sibling指向与自己同级的右侧相邻的兄弟节点。left-child为空表示其没有子节点，right-sibling为空表示其为同级中最右侧节点。&emsp;&emsp;树还有很多表示方法，等到后面学其他数据结构的时候再进行总结。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归分治]]></title>
    <url>%2F2019%2F02%2F01%2F%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%2F</url>
    <content type="text"><![CDATA[递归分治&emsp;&emsp;在之前的归并排序中，使用了递归分治的方法，整个算法经历了三个过程。&emsp;&emsp;分解：将原问题划分为一个个子问题，子问题与原问题形式一样，只是规模更小。&emsp;&emsp;求解：递归的求出子问题。如果子问题规模足够小，停止递归，直接求解。&emsp;&emsp;合并：将子问题的解组合成原问题的解。&emsp;&emsp;当问题足够大时，需要递归求解，这就叫做递归情况，当问题已经分解到足够小时并且不用再递归时，递归已经“触底”，进入基本情况。 1.递归式&emsp;&emsp;递归与分治紧密相关，递归式可以很清楚的刻画分治的运行时间，递归式是一个等式或者一个不等式，可以用更小输入上的一个函数值来描述一个函数。在算法分析中，我用递归式描述了归并排序最坏运行时间T(n)。 $$T(n) =\begin{cases} Θ(1)&amp;n = 1\ 2T(n/2)+ Θ(n)&amp;n &gt; 1 \end{cases}$$ 求解后得到T(n) = $\Theta$(nlgn)。&emsp;&emsp;递归并不是一定要等分，也可以将问题划分为规模不等的子问题，如2/3与1/3的划分，分解和合并都是线性的，这样的递归式就是T(n) = T(2n/3)+T(n/3)+$\Theta$(n)。&emsp;&emsp;有三种求解递归式的方法，可以得出算法为“$\Theta$”或“O”的渐进界方法。 1.1 代入法&emsp;&emsp;这个方法比较玄学，因为第一步需要猜，首先需要猜出解的形式，然后用数学归纳法求出解的常数，并证明解是正确的。当将归纳假设应用于较小值时，将猜测的函数代入，所以叫“代入法”。但是首先我们要猜对解的形式。&emsp;&emsp;由于这个方法并没有一个通用的公式，解出来基本靠经验和运气，所以不再往深处讨论。细节部分翻《算法导论》。 1.2 递归树法&emsp;&emsp;画出递归树是一个非常直观的方法，像在算法分析时为归并排序画出的递归树一样，在树中，每一个节点代表每一个子问题的代价，子问题对应某一次递归函数的调用。将树中每层的代价求和，可以得到每一层的代价，将所有层代价求和，可以得到递归树的总代价。&emsp;&emsp;递归树可以生成猜测，然后用代入法验证猜测是否正确，如果在画递归树时代价求和十分准确，那么也可以直接用递归树证明解是否正确。 1.3 主方法&emsp;&emsp;主方法为递归式提供了一个通式 T(n) = aT(n/b)+f(n) 其中a &gt;= 1、b &gt; 1且都是常数，f(n)是渐进正函数。这个公式描述了一个算法的运行时间：他将规模为n的问题分解为a个子问题，每个子问题规模为n/b，其中a和b都是正常数。a个子问题递归的求解，每个花费时间为T(n/b)。函数f(n)包含了问题分解和子问题解合并的代价。因为n/b不一定为整数，所以一般将T(n/b)替换为T($\lfloor$n/b$\rfloor$)或者T($\lceil$n/b$\rceil$)。，并不会影响渐进式性质。 主定理：令a &gt;= 1和b &gt; 1是常数，f(n)是一个函数，T(n)是定义在非负整数上的递归式 T(n) = aT(n/b)+f(n)其中将n/b解释为$\lfloor$n/b$\rfloor$或者$\lceil$n/b$\rceil$。T(n)有如下渐进界。&emsp;&emsp;1.若某个常数$\epsilon$ &gt; 0 有f(n) = O(nlogba-$\epsilon$)，则T(n) = $\Theta$(nlogba)&emsp;&emsp;2.若f(n) = $\Theta$(nlogba)，则T(n)=$\Theta$(nlogbalgn)&emsp;&emsp;3.若对某个常数$\epsilon$ &gt; 0有f(n) = Ω(nlogba+$\epsilon$)，且对某个常数c &lt; 1和所有足够大的n有af(n/b) &lt;= cf(n)，则T(n) = $\Theta$(f(n))。 &emsp;&emsp;先来尝试理解一下主定理的含义，这三种情况都是将函数f(n)与函数nlogba比较。直觉上，两个函数的较大者决定了递归式的解。nlogba更大，就是情况1，解为T(n)=$\Theta$(nlogba)。如果f(n)更大，就是情况3，解为T(n) = $\Theta$(f(n))。若两个函数相当，就是情况2，要再乘上一个对数因子解为T(n)=$\Theta$(nlogbalgn)=$\Theta$(f(n)lgn)。&emsp;&emsp;第一种情况下并不是f(n)小于$\Theta$(nlogba)就够了，而是要多项式的意义上小于。f(n)必须渐进的小于nlogba，要相差一个因子n$\epsilon$，$\epsilon$是大于0的常数。第三种情况与第一种相似，并且要满足“正则“条件af(n/b)&lt;=cf(n)。&emsp;&emsp;这三种情况并未覆盖所有可能性，三种情况之间都有一定的缝隙，比如f(n)和nlogba并不是形式意义上的大于或小于关系，这种情况无法使用主定理。 主方法使用 &emsp;&emsp;使用主定理时只需要确定情况即可得到解。举个栗子 T(n) = 9T(n/3)+n 对于这个递归式，a = 9,b = 3,f(n) = n,所以nlogba = nlog39 = $\Theta$(n2)。f(n) = O(nlog39-$\epsilon$)，其中$\epsilon$ = 1，适用于情况1，得到解T(n) = $\Theta$(n2)。情况2和情况3的解法与此类似。证明部分见《算法导论》。 2. 最大子数组问题&emsp;&emsp;在一个数组中找出最大的非空连续子数组。 2.1 暴力破解&emsp;&emsp;尝试对每一种情况进行组合，n个数中一共有C(n,2)种组合，C(n,2) = $\Theta$(n2)，每次操作至少也是个常量，所以这种方法的运行时间为Ω(n2).很显然，暴力破解代价有点大。&emsp;&emsp;从另外一个角度来看看输入的数据。目的是找到一段数据，使其从开头到结尾的净变化值最大，所以现不看每次输入的数据，而是每次输入数据的变化值，定义第i个数的变化为第i个数与第i-1个数之差，将其看作数组A，问题就转化为数组A的和最大非空连续子数组，这样的连续子数组为最大子数组，乍一看对于一个n个数的串，我们还是需要检查C(n-1,2)=$\Theta$(n2)个子数组，重新组织计算方式，利用之前计算出的子数组和来计算当前子数组的和，使得每个子数组的和计算时间为O(1)，使暴力破解花费的时间为$\Theta$(n2)，代价还是有点大，我们需要将运行时间限制在o(n2)。 2.2 分治法&emsp;&emsp;首先说明，只有当数组中包含负数时，最大子数组大问题才有意义，否则，最大子数组直接就是数组A。&emsp;&emsp;假定我们寻找数组A的最大子数组，我们需要将数组划分为两个规模大致相同的两个子数组，也就是说找到中央位置，然后将其对半拆分，在数组A[low..high]中的任何子数组必定满足一下条件。 完全位于数组A[low..mid]中，low &lt;= i &lt;= j &lt;= mid. 完全位于数组A[mid+1..high]中，mid &lt; i &lt;= j &lt;= high 跨越中点，low &lt;= i &lt;= mid &lt;= j &lt;= high &emsp;&emsp;最大子数组也必定在这三种情况之内，我们可以递归求解A[low..mid]和A[mid+1..high]的最大子数组，这两个子问题与原问题一样，只是规模更小。剩下的工作就是寻找跨越中点的最大子数组，然后在三者之间挑选值最大者。此时问题加入了限制，子数组必须跨越中点，任何跨越中点的子数组都由A[i..mid]和A[mid+1..j]组成，只要找出形如A[i..mid]和A[mid+1..j]的最大子数组，然后合并即可，下面是该操作的C++实现。1234567891011121314151617181920212223242526272829303132333435int verylow=numeric_limits&lt;int&gt;::lowest();;class node&#123; public: int left; int right; int sum;&#125;;node find_max_crossing_subarray(int *A,int low,int mid,int high)&#123; int left_sum=verylow; int max_left,right_sum,max_right; int sum=0; for(int i = mid; i &gt;= low; i--)//求出左半侧最大子数组 &#123; sum=sum+A[i];//累计求和 if(sum&gt;left_sum)&#123;//逐个相加筛选最大和 left_sum=sum; max_left=i;//记录下标 &#125; &#125; right_sum=verylow; sum=0; for(int j = mid+1; j &lt;= high; j++)//求右侧最大子数组，与上同 &#123; sum=sum+A[j]; if(sum&gt;right_sum)&#123; right_sum=sum; max_right=j; &#125; &#125; node message; message.left=max_left; message.right=max_right; message.sum=left_sum+right_sum; return message;&#125; &emsp;&emsp;如果子数组包含n个元素(n=high-low+1)，调用find_max_crossing_subarray将花费$\Theta$(n)的时间，两次for循环每次花费$\Theta$(1)，第一个for循环执行了mid-low+1次迭代，第二次循环执行了high-mid次迭代，总循环迭代次数为 (mid-low+1)+(high-mid) = high-low+1 = n find_max_crossing_subarray是线性时间的，现在可以对最大子数组进行分治。1234567891011121314151617181920212223node find_maximum_subarray(int *A,int low,int high)&#123; if (high==low)&#123;//数组只有一个元素，本身就是最大和 node n; n.left=low; n.right=high; n.sum=A[low]; return n; &#125; node left_node,right_node,cross_node; int mid=(low+high)/2;//子数组划分 //递归求解左右最大子数组 left_node=find_maximum_subarray(A,low,mid);//左子数组 right_node=find_maximum_subarray(A,mid+1,high);//右子数组 cross_node=find_max_crossing_subarray(A,low,mid,high);//跨越中点的最大子数组，可以看作合并 if(left_node.sum&gt;=cross_node.sum&amp;&amp;left_node.sum&gt;=right_node.sum)&#123;//判断，返回最大子数组 return left_node;//最大子数组在左边 &#125;else if(right_node.sum&gt;=left_node.sum&amp;&amp;right_node.sum&gt;=cross_node.sum) &#123; return right_node;//最大子数组在右边 &#125;else&#123; return cross_node;//最大子数组跨越中点 &#125;&#125; 2.3 分治算法分析&emsp;&emsp;在find_maximum_subarray中，首先现花费常量的时间对n = 1做时间分析，直接得出T(1) = $\Theta$(1)。&emsp;&emsp;当n &gt; 1时开始递归。12行和13行求解的问题均为n/2个元素的子数组，所以每个子数组求解的时间为T(n/2),因为需要求左子数组和右子数组两个问题，所以运行总时间为2T(n/2)，14行调用find_max_crossing_subarray花费了$\Theta$(n)，后面的判断仅仅花费了$\Theta$(1)。因此可以列出等式； T(n) = $\Theta$(1)+2T(n/2)+$\Theta$(n)+$\Theta$(1) = 2T(n/2)+$\Theta$(n) 与上面的等式进行组合，可以得到： $$T(n) =\begin{cases} Θ(1)&amp;n=1\ 2T(n/2)+ Θ(n)&amp;n&gt;1 \end{cases}$$ 和归并排序的递归式一样。&emsp;&emsp;接下来用主方法来求解这个递归式。对于递归式： T(n) = 2T(n/2)+$\Theta$(n) a = 2，b = 2，f(n) = $\Theta$(n)，因此nlogba = nlog22 = n。因为f(n) = $\Theta$(n)，对应情况2，于是得到解T(n) = $\Theta$(nlgn)。我们得到了一个复杂度优于暴力破解的算法，递归分治可以给出一个渐进最快的算法，但是有时候还会有更快的算法，比如最大子数组问题还存在着一个速度更快的线性算法，等到学习总结到了动态规划再对该方法讨论。 &emsp;&emsp;《算法导论》中还有一个矩阵乘法的Strasson算法，虽然知道怎么写，也明白那样做可以减少一次矩阵运算，但是想不出来为什么把矩阵这么拆分组合，等我想通了再更新一下。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渐进记号]]></title>
    <url>%2F2019%2F01%2F31%2F%E6%B8%90%E8%BF%9B%E8%AE%B0%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[渐进记号&emsp;&emsp;在算法分析里面对算法运行时间以及效率问题分析时，当n的规模变得足够大时，我们引入了渐进分析记号$\Theta$来描述当输入规模无限大时，算法运行时间如何随输入规模变大而变大，现在来记录一下另外几种渐进记号约定，用来刻画算法某个其他其他方面的函数(空间与时间)。 &emsp;&emsp;在了解算法运行时间时，我们有时也要在意到底需要哪个运行时间，不是所有时候都需要最坏运行时间，我们希望有一个综合性的来刻画任何输入的运行时间，所以我们需要更多的渐进记号。 1. $\Theta$记号&emsp;&emsp;在分析插入排序与归并排序的运行时间时引入了这个记号插入排序的最坏运行时间为T(n)=$\Theta$(n2)，而归并排序只有T(n)=$\Theta$(nlgn)。&emsp;&emsp;来重新定义一下这个记号，对于一个给定的g(n)，用$\Theta$(n)表示一个函数集合： $\Theta$(g(n)) = {f(n):存在正常量c1、c2和n0，使得所有n &gt;= n0,有0 &lt;= c1g(n) &lt;= f(n) &lt;= c2g(n)} &emsp;&emsp;若存在正常量c1、c2能够在n足够大时，让函数夹在c1g(n)与c2g(n)之间，则f(n)就属于$\Theta$(g(n)),这里的$\Theta$(g(n))是一个函数集合，而非一个函数，所以我在算法分析里简单的把$\Theta$记为了去掉低阶项，忽略系数，这是一个非形式化的概念。在下面的图像中可以看出f(n)高于c1g(n)或者低于c2g(n)，换一句话说，对于所有的n&gt;=n0,f(n)存在一个常量因子等于g(n)，我们称g(n)为f(n)的渐进紧确界。 &emsp;&emsp;$\Theta$(g(n))要求其内的每一个成员f(n)都渐进非负，即n足够大时，f(n)非负(g(n)本身也必须非负，否则集合$\Theta$(g(n))为0)，这个要求对其他符号也成立。 $\Theta$符号有一个活用，当出现$\Theta$(1)时，并没有什么变量趋于无穷，所以$\Theta$(1)用来指任意一个常量或者某个关于变量的常量函数，其他符号也一样。 2. O记号&emsp;&emsp;当我们只需要一个函数的渐进上界的时候，我们采用O记号来表示。对于给定的函数g(n)，用O(g(n))来表示一下函数集合： O(g(n)) = {f(n)：存在正常量c和n0，使得对所有n &gt;= n0，有0 &lt;= f(n) &lt;= cg(n)}。 &emsp;&emsp;用O给出了一个函数在常量因子中的一个上界，从图中可以看出，在n0及其有边的所有n值中，f(n)总是小于或者等于cg(n)。 &emsp;&emsp;f(n)=$\Theta$(g(n))是一个比f(n)=O(g(n))更强的概念，f(n)=$\Theta$(g(n))包含f(n)=O(g(n))，按照集合论的方法，写作$\Theta$(g(n)) $\subseteq$ O(g(n))，所以如果证明了任意二次函数 an2+bn+c a &gt; 0 如果证明了该函数在$\Theta$(n2)中，也就证明了这个函数在O(n2)中，任意线性函数an+b a &gt; 0也在O(n2)中。&emsp;&emsp;O记号可以仅仅通过检查算法的总体结构来描述算法的运行时间，例如插入排序的双重循环结构中就会产生一个O(n2)的上界：内层循环的每一次代价至多为O(1)，下标i与j均最多为n，双重循环会产生至多n2次访问。&emsp;&emsp;O通常用来修限制算法的最坏运行时间，在每一次输入时，对于插入排序的最坏时间O(n2)适用于每一个输入。但是在插入排序最坏运行时间$\Theta$(n2)并不意味着每一次输入运行时间界都是$\Theta$(n2)，例如当输入已经被排序时，这时候运行时间界就是$\Theta$(n)。&emsp;&emsp;一般当我们说“运行时间为O(n2)”时，指存在一个O(n2)的函数f(n)，使得对任意n，不管选择什么特定规模的n输入，运行时间上界都是f(n)，这就是运行的最坏情况时间为O(n2)。 3. Ω记号&emsp;&emsp;有运行时间上界，当然对应的也有运行时间下界，称为函数的渐进下界，用符号Ω表示，对于给定的函数g(n)，用Ω(g(n))来表示一下函数集合： Ω(g(n)) = {f(n):存在正常量c&lt;和n0，使得所有n&gt;=n0,有0 &lt;= cg(n) &lt;= f(n)} 下图可以直观解释Ω记号的含义，这里不说太多。 &emsp;&emsp;根据上面三个符号的定义，很容易得出一个定理 对于任意两个函数f(n)与g(n)，有f(n)=$\Theta$(g(n))，当且仅当f(n) = O(g(n))且f(n) = Ω(g(n))。 &emsp;&emsp;当存在二次函数an2+bn+c a &gt; 0时，有an2+bn+c = $\Theta$(n)时，同时说明了an2+bn+c = O(n)、an2+bn+c = Ω(n)，通常用渐进上界与渐进下界来证明渐进紧确界，当一个算法的运行时间为Ω(g(n))时，对于每个值，不管选择什么输入，n足够大时，他的时间都是g(n)的整数倍，说插入排序的最好运行时间为Ω(n)，蕴含着插入排序的运行时间为Ω(n)。&emsp;&emsp;所以称插入排序的运行时间介于Ω(n)和O(n2)之间，落入n的线性函数与n的二次函数之间的任何地方。插入排序运行时间不是Ω(n2)因为存在输入使得其能在$\Theta$(n)时间内运行。如果说插入排序运行最坏运行时间为Ω(n2)并不错误，因为存在一个输入使得该算法运行时间为Ω(n2)。 4. 等式与不等式&emsp;&emsp;在渐进记号中的等号两端并非等价的，一般渐进记号在等式(或不等式)右端时，通常指集合的成员关系，比如n = O(n2)指n $\in$ O(n2)，我们将公式中的渐进记号解释为不带名称的匿名函数。入2n2+3n+1 = 2n2+$\Theta$(n),这里的$\Theta$(n)指在$\Theta$(n)集合之内的一个函数。&emsp;&emsp;这种方式可以帮我们消除等式中一些无关紧要的细节，如果只对某个算法的渐进行为感兴趣，就没必要说明所有的低阶项，都被理解为包含在了$\Theta$(n)表示的匿名函数中。&emsp;&emsp;某些情况下渐进记号可能出现在等式左边，如： 2n2+$\Theta$(n) = $\Theta$(n2) 无论怎样选择等号左边的匿名函数，总有一种办法来选择等号右边的匿名函数使等式成立。在2n2+$\Theta$(n) = $\Theta$(n2)中我们可以取$\Theta$(n)集合中的3n+1，使等式变为2n2+3n+1 = $\Theta$(n2)，这样2n2+$\Theta$(n)就在$\Theta$(n2)集合之内，可以说等式左边提供的细节比等式右边更粗糙。 5. o记号与ω记号&emsp;&emsp;O记号与Ω记号可能并非渐进紧却的，比如2n2 = O(n2)是渐进紧却的，而2n = O(n2)就不是渐进紧却的，我们用小写的o与ω定义集合。 o(g(n)) = {f(n):对任意正常量c&gt;0,存在常量n0&gt;0，使得所有n &gt;= n0,有0 &lt;= f(n) &lt; cg(n)},如2n = o(n2)但2n2 $\neq$ o(n2)。 ω(g(n)) = {f(n):对任意正常量c &gt; 0,存在常量n0 &gt; 0，使得所有n &gt;= n0,有0 &lt;= cg(n) &lt; f(n)}，如n2/2 = ω(n),而2n $\neq$ ω(n)。 &emsp;&emsp;这两个符号都是非渐进紧却的，与O记号和Ω记号最大的不同是，o记号与ω记号并不能等于g(n)关于c常量的函数。 f(n) $\in$ ω(g(n)),当且仅当 g(n) $\in$ o(f(n)) &emsp;&emsp;将f与g类比为实数a和b。 记号 含义 类比 $\Theta$ 渐进紧却界 a = b O 渐进上界 a &lt;= b Ω 渐进下界 a &gt;= b o 非紧却上界 a &lt; b ω 非紧却下界 a &gt; b 6. 函数比较&emsp;&emsp;实数的很多性质都适用于函数的渐进比较，设f(n)和g(n)渐进为正。 6.1 传递性 f(n) = $\Theta$(g(n)) 且 g(n) = $\Theta$(h(n)) 蕴含 f(n) = $\Theta$(h(n))f(n) = O(g(n)) 且 g(n) = O(h(n)) 蕴含 f(n) = O(h(n))f(n) = Ω(g(n)) 且 g(n) = Ω(h(n)) 蕴含 f(n) = Ω(h(n))f(n) = o(g(n)) 且 g(n) = o(h(n)) 蕴含 f(n) = o(h(n))f(n) = ω(g(n)) 且 g(n) = ω(h(n)) 蕴含 f(n) = ω(h(n)) 6.2 自反性 f(n) = $\Theta$f(n)f(n) = Of(n)f(n) =Ωf(n) 6.3 对称性 f(n) = $\Theta$(g(n)) 当且仅当 g(n) = $\Theta$(f(n)) 6.4 转置对称性 f(n) = O(g(n)) 当且仅当 g(n) = Ω(f(n))f(n) = o(g(n)) 当且仅当 g(n) = ω(f(n)) 6.5 三分性&emsp;&emsp;对于任意两个数a、b，有三种情况必须成立：a &gt; b、a &lt; b、a = b。任意两个实数可以比较，但不是任意两个渐进函数都可以比较。对于g(n)与f(n)，f(n) = O(g(n))和f(n) = Ω(g(n))可能都不成立。例如n与n1+sin n，因为n1+sin n的幂值在0到2之间来回摆动。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法分析]]></title>
    <url>%2F2019%2F01%2F24%2F%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[算法分析 The analysis of algorithm is the theoretical study of computer program performance and resource usage算法分析是理论研究关于计算机程序性能和资源利用的研究。 &emsp;&emsp;这一句话是MIT公开课上Charles Leiserson第一节课对于算法分析的定义。学习算法是为了如何让计算机更快，也就是解决计算机的性能问题。当然，在目前软件工程领域来讲，性能已经不是第一位了，软件的可维护性、健壮性、安全性、可扩展性、模块化、用户友好性、更多的功能都非常重要。为什么很多人更加青睐于MacOS而非Windows？因为用户友好性以及安全性，在计算机发展史上，这是一个非常大的飞跃。&emsp;&emsp;但是，所有的这些事情看似都比性能重要，我们为什么要学算法？ 1.性能与用户体验是紧密联系的。 &emsp;&emsp;打开一个应用需要等半天是不会有什么用户体验的，即便界面再漂亮也没有用。所以性能达不到需求，用户体验很难保证。 2.性能直接决定了可行与不可行。 &emsp;&emsp;程序太慢或者程序占用内存太大会直接导致程序在一些机器上不可行。 3.算法是描述程序行为的一种语言。 &emsp;&emsp;这是一门被计算机学科广泛使用的语言，已经被所欧的实践者所采用的理论语言，是程序的思考方式。Charles Leiserson用一个很有趣的例子去描述了性能为什么处于最底层。他把性能比做经济中的货币，而可维护性、用户体验是食物和水，虽然食物和水对于生命来说比金钱更重要，但他们都需要货币去换取，性能是他们的保障。这就是为什么那么多人用Java去写程序而非C，尽管前者比后者多耗费了三倍的性能，因为Java的良好特性值得人们去付出这些代价，在这里，性能就被充当为了货币去获取其他的对象，这就是性能处于底层的原因。 4.很有趣。 现在就从一个非常简单的排序问题引入算法分析。 排序问题&emsp;&emsp;这是一个很古老的问题，输入一组序列a1，a2……an。按需求后输出a1‘&lt;=a2‘&lt;=……&lt;=an‘。使得他们单调递增。 插入排序&emsp;&emsp;首先引入插入排序，插入排序的算法思想，数组从左到右将其中每一个元素依次设为key，key的左半部分是已经排序过的数组，每一次遍历就会将已排序部分增长一，而每一次循环将会把已排序数组的值一个一个向右移动，知道出现合适位置，并将key插入其中，所以这个排序称之为插入排序。下面是对《算法导论》中插入排序伪代码的C++实现。123456789101112void insertSort(int *a,int length)&#123; int key,i; for(int j=1;j&lt;length;j++)&#123;//从第二个开始遍历，直到最后一个 key=a[j];//标记当前需要插入的数 i=j-1;//i作为指针，遍历j之前的数 while(i&gt;=0&amp;&amp;a[i]&gt;key)&#123; //开始遍历，如果遍历到的数比key大，就把这个数向后移一位，知道循环完毕或者碰到比key小的数 a[i+1]=a[i]; i=i-1; &#125; a[i+1]=key;//将key插入 &#125; 这就是插入排序，现在我们对这个排序进行一个分析，首先关注运算时间问题。 1.输入本身 如果输入的数组本身有序，那么我们做的事将会很少，但是最坏情况下，也就是逆序情况下，我们将会得到最坏对运行时间。 2.输入规模 我们在输入几个元素的情况下，这个算法很快，但是如果输入几十亿个元素对话，就需要很长时间，所以输入规模越大，运行时间越长。 3.运行时间上界 这代表对用户的承诺，就是是说保证这个程序运行不会超过某个时间，相反的，要说说这个程序运行至少多长时间，这种信息就不能被当作承诺。 算法分析平均情况分析&emsp;&emsp;有些情况下会这么做，在这里T(n)就是输入规模为n下所有可能输入的期望时间，也就是概率论中所说的E(x)，但是我们需要一个输入统计的分布假设，否则期望就无从谈起，最常见的是均匀分布。 最好情况分析&emsp;&emsp;一般被称为假象，因为最好情况基本很少出现，即使出现也是仅仅针对于少量情况适用，比如对一个已排好序的数组排序，但是不包含大量其他情况。 最坏情况分析&emsp;&emsp;这是我们通常最关注的问题，我们首先设T(n)为输入规模为n时的最长运行时间，只有做最坏的打算，我们才能对我们的程序进行承诺。&emsp;&emsp;先看一下插入排序的最坏时间。 首先取决于运行他的计算机。 一般比较算法比较的是他的相对运算速度也就是同一台机子上的运行速度，所以这个一般不考虑。我们需要计算机的大局观，一种观点，就是渐进分析。 渐进分析&emsp;&emsp;渐进分析的思路是忽略掉那些依赖于机器的常量，以及不是去检查实际的运行时间，而是关注运行时间增长。&emsp;&emsp;首先采用渐进符号$\Theta$。 弃去公式的低阶项，并忽略前面的常数因子，如果公式是3n3+90n2-5n+6046,那么这个公式可以说是$\Theta$(n3)。 当n趋近于无穷大时，$\Theta$(n2)肯定比$\Theta$(n3)快，无论其他项是什么也动摇不了这个结果，即使在一台慢速计算机运行$\Theta$(n2)，在一台高速计算机运行$\Theta$(n3)。但是这两个函数相交于一个n0点，如果这个n0过大，计算机就无法计算，所以某些时候会青睐一些低速算法，尽管从渐进角度看，他们会比较慢，但在合理输入情况下他们可能会更快。&emsp;&emsp;继续分析插入排序，在逆向排序好的情况下，插入排序需要将所有项全部翻一遍，假设每一个操作耗费一个原子时间，循环中所有的操作都是原子时间，所以在上面的算法中，j从1循环到length，我们可以写成数学中的求和符号，从1到length,设length为n,对于一个给定的j值，循环将会进行$\Theta$(j)次操作，因为内层循环里的i以j-1为初值，在i每次取新值时进行固定数量的操作，i从j-1降到0。 T(n)=$\sum_{j=1}^n$$\Theta$(j)=$\Theta$(n2)连续整数求和，算术级数 对于很小的n，插入排序很快，但对于巨大的n，就不行了，我们需要一个更快的算法————归并排序。 归并排序&emsp;&emsp;首先对于数组A[1……n] 1.如果n为1，那么数组就是排好序的。2.递归递归的对A[1到n/2向上取整]的这一部分，以及A[n/2+1向上取整到n]这部分排序3.把排好序的两个表归并 归并过程如图所示实质上就是将两个已排序好对的数组，也就是A[1..N/2]和A[N/2+1..n]，进行归并，得到一个排序好的数组，每一步都是固定的操作与数组长度无关，所以对于总数为n的输入，时间是$\Theta$(n)的。&emsp;&emsp;对于这个递归，我们可以写出一个递推式。 $$T(n) =\begin{cases} Θ(1)&amp;n=1\ 2T(n/2)+ Θ(n)&amp;n&gt;1 \end{cases}$$ &emsp;&emsp;这是一个树状结构，树的末端时间只有$\Theta$(1)，而树的高度是lgn，叶的节点总数为n，完全扩展的递归树有lgn+1层，每层贡献总代价$\Theta$(n)，如果计算总数就是$\Theta$(n)lgn+$\Theta$(n)，根据渐进的思想，最后结果就是$\Theta$(nlgn)，考虑渐进，他比$\Theta$(n2)快,在数据足够大的情况下，归并排序将优于插入排序，差不多n大于30归并就更快了。下面是对《算法导论》中归并排序的C++实现。1234567891011121314151617181920212223242526272829303132333435const int N=204800;void Merge(int *arr,int p,int q,int r)&#123; int n1=q-p+1;//左数组长度 int n2=r-q;//右数组长度 int left[n1+1],right[n2+1];//开辟新的左右数组 for(int i = 0; i !=n1; ++i)&#123; left[i]=arr[p+i];//为左数组赋值 &#125; left[n1]=N;//左数组“哨兵” for(int j = 0; j!= n2; ++j) &#123; right[j]=arr[q+j+1];//为右数组赋值 &#125; right[n2]=N;//右数组“哨兵” int i=0,j=0; for(int k = p; k !=r+1; ++k)//将左右数组归并至原数组 &#123; if(left[i]&gt;right[j])&#123; arr[k]=right[j]; ++j; &#125;else&#123; arr[k]=left[i]; ++i; &#125; &#125;&#125;void MergeSort(int *arr,int p,int r)&#123; //分治法，将数组分割，将复杂问题化简为数个简单问题 if(p&lt;r)&#123; int q=(p+r)/2;//数组分割标记，中间下标 MergeSort(arr,p,q);//分割左边数组 MergeSort(arr,q+1,r);//分割右边数组 Merge(arr,p,q,r);//进行归并排序 &#125;&#125; 这就是算法分析的一部分，后面还有更多的分析方法，如果想知道更多的证明细节，详见《算法导论》。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-使用QueryDSL查询框架]]></title>
    <url>%2F2019%2F01%2F22%2FSpringBoot-%E4%BD%BF%E7%94%A8QueryDSL%E6%9F%A5%E8%AF%A2%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[SpringBoot-使用QueryDSL查询框架(基础) QueryDSL是一个通用的查询框架，框架的核心原则是创建安全类型的查询，开始QueryDSL仅支持Hibernate（HQL），在不断开源人士加入QueryDSL团队后，陆续发布了针对JPA，JDO，JDBC，Lucene，Hibernate Search，MangoDB，Collections 和RDF(Relational Data Format) Bean等。 简述&emsp;&emsp;在使用SpringDataJPA的时候，内置的QueryByExampleExecutor对复杂查询来说显然不怎么好用，于是在这里引入一个便于我们进行复杂查询的框架————QueryDSL。这个框架可以完美的和SpringDataJPA进行融合，接下来主要说怎么用。 项目构建&emsp;&emsp;怎么搭建SpringBoot、数据源、数据库连接、配置SpringDataJPA就不说了，在原先的基础上添加项目依赖。12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;com.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-jpa&lt;/artifactId&gt; &lt;version&gt;4.0.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-apt&lt;/artifactId&gt; &lt;version&gt;4.2.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.inject&lt;/groupId&gt; &lt;artifactId&gt;javax.inject&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt;&lt;/dependency&gt; 仅仅添加依赖还不够，我们需要在下面的&lt;plugins&gt;标签中添加插件 1234567891011121314151617&lt;!--该插件生成查询对象--&gt; &lt;plugin&gt; &lt;groupId&gt;com.mysema.maven&lt;/groupId&gt; &lt;artifactId&gt;apt-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;process&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;target/generated-sources/java&lt;/outputDirectory&gt; &lt;processor&gt;com.querydsl.apt.jpa.JPAAnnotationProcessor&lt;/processor&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 然后在idea界面右上角打开Edit Configurations,新建一个maven，填入如图所示信息。在右上角启动这个插件。该插件会自动扫描项目内配置了@Entity的实体类，并根据实体的内定义的字段以及关联类通过JPAAnnotationProcessor自动创建Q[实体类名称]的查询实体，创建完成后会将实体存放到我们配置outputDirectory属性目录下。如图所示。 准备准备数据库信息&emsp;&emsp;先构建一个普通的User实体类,项目启动时会自动根据这个实体类建表，建表后向里面扔几条测试数据。123456789101112131415161718192021/** * @Author: cp * @Date: 2019/1/21 下午11:13 * @Version 1.0 */@Entity@Table(name = "user")public class User implements Serializable &#123; @Id @Column(name = "id") @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; @Column(name = "name") private String name; @Column(name = "age") private String age; @Column(name = "address") private String address; @Column(name = "pwd") private String pwd; /*setter and getter*/ 创建基类JPA&emsp;&emsp;正如上一篇所说，正常情况下一个项目肯定不可能就继承一个JpaRepository接口，再使用其他模块时还需要多个接口继承，如果每一个业务数据接口都继承几个相同的接口的话，对于系统设计和代码复用性来说不是个什么好的选择。所以我们需要一个基类JPA。12345678/** * @Author: cp * @Date: 2019/1/21 下午11:16 * @Version 1.0 */@NoRepositoryBeanpublic interface BaseJPA&lt;T&gt; extends JpaRepository&lt;T,Integer&gt;,JpaSpecificationExecutor&lt;T&gt;,QuerydslPredicateExecutor&lt;T&gt; &#123;&#125; @NoRepositoryBean是为了避免自动实例化。 创建对应模块JPA&emsp;&emsp;创建对应User模块的数据逻辑接口JPA，因为BaseJPA已经继承了我们所需要的所有接口，所以我们只需要继承BaseJPA就可以了。1234567/** * @Author: cp * @Date: 2019/1/21 下午11:18 * @Version 1.0 */public interface UserJPA extends BaseJPA&lt;User&gt; &#123;&#125; 执行插件生成Q结构查询实体&emsp;&emsp;这个不多说了，刚才生成过了。打开自动创建的实体后可以看到QueryDSL自动为我们创建的查询字段以及构造函数。 编写控制器&emsp;&emsp;为了方便我就不写Service层了，所有业务逻辑全部放在Controller层，不过正式开发时一定要写。12345678910111213141516/** * @Author: cp * @Date: 2019/1/21 下午11:20 * @Version 1.0 */@RestControllerpublic class UserController &#123; @Autowired private UserJPA userJPA; @Autowired private EntityManager entityManager; private JPAQueryFactory queryFactory; @PostConstruct public void initFactory()&#123; queryFactory=new JPAQueryFactory(entityManager); &#125; 声明EntityManager的注入以及JPAQueryFactory工厂对象的创建，通过@PostConstruct注解在类初始化的时候完成对JPAQueryFactory对象的实例化。 查询查询并排序12345678@RequestMapping(value = "/queryAll")public List&lt;User&gt; queryAll()&#123; QUser qUser=QUser.user;//创建查询对象 return queryFactory//查询并返回 .selectFrom(qUser)//源 .orderBy(qUser.id.desc())//根据id倒序 .fetch();//执行并返回结果集&#125; &emsp;&emsp;queryAll方法内首先获取了对应UserBean的查询实体QUserBean，通过QUserBean内自动生成的字段获取，我们使用JPAQueryFactory工厂对象的selectFrom方法来简化查询，该方法代替了select&amp;from两个方法，注意：也是仅限单表操作时可以使用。&emsp;&emsp;这样写起来感觉就像在写原生SQL，在一系列的条件都添加完成后，调用fetch方法执行我们的条件查询并且获取对应selectFrom查询实体的类型集合，要注意一点：这里如果selectFrom参数的实体类型不是User那fetch方法返回集合的类型也不是List&lt;User&gt;。&emsp;&emsp;启动后访问这个方法就可以在网页中见到这些数据，同时在控制台中也可以看见生成的SQL语句。1234567891011Hibernate: select user0_.id as id1_3_, user0_.address as address2_3_, user0_.age as age3_3_, user0_.name as name4_3_, user0_.pwd as pwd5_3_ from user user0_ order by user0_.id desc 根据主键查询数据&emsp;&emsp;这个方法是最常用的方法，现在我们有两种写法，一种是使用QueryDSL，另外一种是和SpringDataJPA结合。使用QueryDSL：12345678@RequestMapping(value = "/detail/&#123;id&#125;")public User detail(@PathVariable("id") Integer id)&#123; QUser qUser=QUser.user;//使用querydsl return queryFactory .selectFrom(qUser)//源 .where(qUser.id.eq(id))//条件 .fetchOne();//结果&#125; SpringDataJPA+QueryDSL:12345@RequestMapping(value = "/detail_2/&#123;id&#125;") public Optional&lt;User&gt; detail_2(@PathVariable("id") Integer id)&#123; QUser qUser=QUser.user;//使用querydsl return userJPA.findOne(qUser.id.eq(id));//查询并返回结果 &#125; 网上的一些教程关于SpringDataJPA+QueryDSL的方法返回值好多都是直接返回实体类，不过最新的方法已经修改为返回Optional类，用来解决万恶的空指针异常。&emsp;&emsp;这两种代码效果是一样的，都是根据主键返回对应信息，看起来SpringDataJPA+QueryDSL还简单一点，不过这只限于单表查询，一旦涉及各种复杂操作还是QueryDSL简单。在使用QueryDSL查询指定主键时，我们使用了where方法并且指定了id字段需要eq参数id，这个eq是QueryDSL内置的一个方法，用于查询指定值数据，当然其他字段也同样可以使用eq方法来完成条件查询，都是可以变通使用的。 模糊查询&emsp;&emsp;根据name字段模糊查询。12345678@RequestMapping(value = "like")public List&lt;User&gt; likeQueryByName(String name)&#123; QUser qUser=QUser.user;//使用querydsl return queryFactory .selectFrom(qUser)//源 .where(qUser.name.like("%"+name+"%"))//条件 .fetch();&#125; like是QueryDSL内置方法，只要存入条件，就可以完成模糊查询，可以看一下控制台输出的SQL语句。1234567891011Hibernate: select user0_.id as id1_3_, user0_.address as address2_3_, user0_.age as age3_3_, user0_.name as name4_3_, user0_.pwd as pwd5_3_ from user user0_ where user0_.name like ? escape '!' 更新使用SpringDataJPA更新实体&emsp;&emsp;与Hibernate一样，SpringDataJPA内置了一个save方法用于保存、更新实体内容，如果存在主键值则更新对应信息，反则是添加一条新信息。我们新添加一个方法。12345678910@RequestMapping(value = "/updateJPA")public String updateJPA()&#123; User user=new User(); user.setId(1); user.setName("王五"); user.setAge("13"); user.setPwd("321"); userJPA.save(user); return "success";&#125; 很简单，并没有什么难理解的地方，直接访问这个方法就可以看结果了，重点是先看控制台输出的SQL语句，首先输出了一条查询语句。12345678910select user0_.id as id1_3_0_, user0_.address as address2_3_0_, user0_.age as age3_3_0_, user0_.name as name4_3_0_, user0_.pwd as pwd5_3_0_ from user user0_ where user0_.id=? 这是因为SpringDataJPA先去查询数据库中有没有这个主键的信息，如果有，才会执行下面的更新语句。123456789update user set address=?, age=?, name=?, pwd=? where id=? 使用QueryDsl更新实体1234567891011121314151617181920@Transactional@RequestMapping(value = "updateDsl")public String updateByQueryDsl()&#123; User user=new User(); user.setPwd("090"); user.setAge("80"); user.setName("赵四"); user.setId(1); user.setAddress("北京"); QUser qUser=QUser.user; queryFactory .update(qUser) .set(qUser.name,user.getName()) .set(qUser.address,user.getAddress()) .set(qUser.age,user.getAge()) .set(qUser.pwd,user.getPwd()) .where(qUser.id.eq(user.getId())) .execute(); return "SUCCESS";&#125; &emsp;&emsp;与SpringDataJPA不同的是，使用save方法的时候，如果实体类中某一条是空的，那么数据库中对应字段也会被设为空，而使用QueryDsl时就可以随心所欲的选择只更新某几条信息。设置完成更新字段后需要设置更新的条件，不设置也是可以的，当然这里肯定跟原生SQL一样，不设置条件就更新表内全部的数据。最后使用execute()执行操作就行了。 敲黑板：在执行update/delete方法时必须添加事务，也就是@Transactional注解，否则会抛出异常。 这是控制台输出的语句。12345678910Hibernate: update user set name=?, address=?, age=?, pwd=? where id=? 删除使用SpringDataJPA删除实体信息1234567@RequestMapping(value = "/deletejpa")public String deletejpa()&#123; User user=new User(); user.setId(4); userJPA.deleteById(user.getId());//删除指定主键的值 return "success";&#125; &emsp;&emsp;简单粗暴，不用解释。 使用QueryDsl删除实体信息123456789101112@Transactional//这个必须有@RequestMapping(value = "/deletequery")public String deleteQueryDsl()&#123; User user=new User(); user.setId(4); QUser qUser=QUser.user; queryFactory .delete(qUser)//删除 .where(qUser.id.eq(user.getId()))//条件 .execute();//执行 return "sueecss";&#125; &emsp;&emsp;跟update方法差不多，就是把update改成delete，看起来好像比SpringDataJPA还麻烦了，但是我们引入这个框架是为了复杂查询，所以我们加一个限定条件，删除id大于四，且name值为王五的人。123456789101112131415@Transactional//这个必须有@RequestMapping(value = "/deletequeryid")public String deletequeryById()&#123; User user=new User(); user.setName("mvm"); QUser qUser=QUser.user; queryFactory .delete(qUser)//删除 .where( qUser.name.eq(user.getName()) .and(qUser.id.gt(4))//条件 ) .execute();//执行 return "success";&#125; SpringDataJPA想要完成这个就没那么简单了，看一下生成的SQL语句。1234567Hibernate: delete from user where name=? and id&gt;? 还是很规范的，输出的SQL完全根据我们设置的条件来自动生成，QueryDsl内的条件可以跟原生SQL完全一样，可以完全采用SQL的思想来编写条件。 最后&emsp;&emsp;以上是SpringDataJPA结合QueryDSL的基础操作，后续还会继续学习多表关联查询、自定义返回对象、聚合查询、子查询等复杂操作。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>SpringDataJPA</tag>
        <tag>QueryDSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-整合SpringDataJPA]]></title>
    <url>%2F2019%2F01%2F21%2FSpringBoot-%E6%95%B4%E5%90%88SpringDataJPA%2F</url>
    <content type="text"><![CDATA[SpringBoot-整合SpringDataJPA1.简述&emsp;&emsp;SpringDataJPA是Spring Data的一个子项目，默认底层是Hibernate，使用基于JPA的Repository,极大的减少对数据库访问的代码量，仅仅使用内部接口就可以完成简单的crud操作。 2.搭建项目&emsp;&emsp;还使用之前搭好的框架进行测试，由于是测试，为了方便就不搭建Service层了，所有逻辑代码都放在了Controller层，数据源使用alibaba的Druid数据源，关于Druid以后慢慢说，不放在这里讨论。 2.1导入依赖并修改配置文件&emsp;&emsp;首先在pom.xml原来的基础中添加配置。123456789&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 然后修改配置文件，添加关于Druid和SpringBootJPA的依赖123456789101112131415161718192021222324252627282930313233343536spring: mvc: view: prefix: /WEB-INF/views/ suffix: .jsp datasource: type: com.alibaba.druid.pool.DruidDataSource url: jdbc:mysql://127.0.0.1:3306/test3?characterEncoding=UTF-8 username: root password: 20141232 driver-class-name: com.mysql.jdbc.Driver #最大活跃数 maxaActive: 20 #初始化数量 InitialSize: 1 #最大连接等待时间 maxWait: 60000 #打开PSCache，并指定大小 poolPreparedStatements: true maxPoolPreparedStatementPerConnectionSize: 20 minIdle: 1 connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: select 1 from dual testWhileIdle: true testOnBorrow: false testOnReturn: false filters: stat, wall, log4j jpa: properties: hibernate: hbm2ddl: auto: update show_sql: true format_sql: true 之后还需要在入口文件ZzuliApplication注解中添加参数。123456@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)public class ZzuliApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZzuliApplication.class, args); &#125;&#125; 禁止掉多数据源的自动注入，如果不这样做就会产生maven的依赖包冲突，导致重复依赖。 2.2 使用JpaRepository&emsp;&emsp;与Hibernate一样，都需要一个实体类对数据库表结构进行映射，在启动项目时，会对实体类创建相应的表结构。所以先在domain包里创建StudentEntity类，get和set方法在此省略了。1234567891011121314151617181920212223242526272829package cn.edu.zzuli.domain;import cn.edu.zzuli.base.BaseEntity;import javax.persistence.*;import java.io.Serializable;/** * @Author: cp * @Date: 2019/1/20 下午3:56 * @Version 1.0 */@Entity@Table(name = "studentEntity")public class StudentEntity implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "s_id") private Integer id; @Column(name = "s_name") private String name; @Column(name = "s_age") private Integer age; @Column(name = "s_address") private String address; /*getter and setter*/&#125; 对这几个注解做一下说明，使用@Entity会对实体类进行持久化操作，当JPA检测到实体类中有@Entity注解时，会在数据库中生成对应的表结构信息。@Table用来指定表名，@Id用来指定主键，配合@GeneratedValue(strategy = GenerationType.IDENTITY)指定主键自增策略，这里将主键自增交给数据库去做，所以使用IDENTITY，@Column用来指定对应表中的字段名。&emsp;&emsp;之后我们需要创建StudentJPA接口,先创建一个jpa包，在下面创建StudentJPA接口，继承JpaRepository，需要两个参数，一个是实体类对象，一个是主键类型。1234567/** * @Author: cp * @Date: 2019/1/20 下午4:01 * @Version 1.0 */public interface StudentJPA extends JpaRepository&lt;StudentEntity,Integer&gt; &#123;&#125; 我们查看JpaRepository就可以看到这个接口又继承了PagingAndSortingRepository和QueryByExampleExecutor接口，PagingAndSortingRepository又继承了CrudRepository接口，浓浓的Spring风格，基本上看名字就知道这个接口大概是干什么的，至于为什么这么设计，那就是架构的问题了。 2.1.1 CrudRepository&emsp;&emsp;看名字就知道，这个接口中包含了crud操作，也就是creat、select、delete、update、exist、count.1234567891011121314@NoRepositoryBeanpublic interface CrudRepository&lt;T, ID&gt; extends Repository&lt;T, ID&gt; &#123; &lt;S extends T&gt; S save(S var1); &lt;S extends T&gt; Iterable&lt;S&gt; saveAll(Iterable&lt;S&gt; var1); Optional&lt;T&gt; findById(ID var1); boolean existsById(ID var1); Iterable&lt;T&gt; findAll(); Iterable&lt;T&gt; findAllById(Iterable&lt;ID&gt; var1); long count(); void deleteById(ID var1); void delete(T var1); void deleteAll(Iterable&lt;? extends T&gt; var1); void deleteAll();&#125; 如果继承该接口就会拥有所有该接口的实现。 2.1.2 PagingAndSortingRepository&emsp;&emsp;顾名思义，分页和排序，而且继承了CrudRepository接口，拥有其所有的接口实现。12345@NoRepositoryBeanpublic interface PagingAndSortingRepository&lt;T, ID&gt; extends CrudRepository&lt;T, ID&gt; &#123; Iterable&lt;T&gt; findAll(Sort var1); Page&lt;T&gt; findAll(Pageable var1);&#125; 2.1.3 QueryByExampleExecutor&emsp;&emsp;这个接口就是实现条件查询和复杂查询的，可以使用exmple的方式查询。12345678public interface QueryByExampleExecutor&lt;T&gt; &#123; &lt;S extends T&gt; Optional&lt;S&gt; findOne(Example&lt;S&gt; var1); &lt;S extends T&gt; Iterable&lt;S&gt; findAll(Example&lt;S&gt; var1); &lt;S extends T&gt; Iterable&lt;S&gt; findAll(Example&lt;S&gt; var1, Sort var2); &lt;S extends T&gt; Page&lt;S&gt; findAll(Example&lt;S&gt; var1, Pageable var2); &lt;S extends T&gt; long count(Example&lt;S&gt; var1); &lt;S extends T&gt; boolean exists(Example&lt;S&gt; var1);&#125; 但是我感觉这个东西不怎么好用，以后会有替代品。 2.1.4 JpaRepository&emsp;&emsp;我们用的就是这个接口，它拥有以上所有接口的方法实现，并添加了条件查询和保存集合数据的方法，基本上简单的数据库操作就不需要写SQL了。1234567891011121314@NoRepositoryBeanpublic interface JpaRepository&lt;T, ID&gt; extends PagingAndSortingRepository&lt;T, ID&gt;, QueryByExampleExecutor&lt;T&gt; &#123; List&lt;T&gt; findAll(); List&lt;T&gt; findAll(Sort var1); List&lt;T&gt; findAllById(Iterable&lt;ID&gt; var1); &lt;S extends T&gt; List&lt;S&gt; saveAll(Iterable&lt;S&gt; var1); void flush(); &lt;S extends T&gt; S saveAndFlush(S var1); void deleteInBatch(Iterable&lt;T&gt; var1); void deleteAllInBatch(); T getOne(ID var1); &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; var1); &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; var1, Sort var2);&#125; 3.测试3.1 创建Controller层&emsp;&emsp;在controller 包中新建JPAStudentController类，这回测试就不使用JSP了，直接带上@RestController注解，并注入StudentJPA，返回json格式验证数据。12345678910/** * @Author: cp * @Date: 2019/1/20 下午4:06 * @Version 1.0 */@RestControllerpublic class JPAStudentController &#123; @Autowired private StudentJPA studentJPA;&#125; 3.2开启测试3.1.1 查询&emsp;&emsp;在JPAStudentController中添加list方法，使用JpaRepository内部实现的findAll方法。1234@RequestMapping(value = "/stu")public List&lt;StudentEntity&gt; list() &#123; return studentJPA.findAll();&#125; 接着在浏览器访问访问localhost:8080/stu，可以看到页面返回的数据。可以看到我在数据库预先准备的数据。&emsp;&emsp;除此之外，只要继承JpaRepository接口，我们还能使用方法规则进行查询，我第一次见的时候感觉挺神奇的，举个栗子，我在StudentJPA中定义StudentEntity findByName(String name);方法，他就可以直接被解析为。1select from StudentEntity where name=? 超级方便，简单的查询直接写定义这么一个方法就好了，不过弊端就是对于复杂查询，方法名会超级长，而且很难实现。&emsp;&emsp;如果想对SQL语句进行细致优化的话，我们还可以使用@Query注解自定义SQL。打开StudentJPA，在其中添加以年龄为条件的查询。12@Query(value = "select * from student_entity where s_age&gt;=?",nativeQuery = true)public List&lt;StudentEntity&gt; SelectByAge(int age); nativeQuery这个设置为true表明使用原生SQL，否则默认启用HQL。在controller层添加代码。1234@RequestMapping(value = "/age")public List&lt;StudentEntity&gt; age()&#123; return studentJPA.SelectByAge(20);&#125; 重启项目并在浏览器中输入localhost:8080/age。 3.1.2 增加&emsp;&emsp;增加数据只需要将实体类当作参数，调用StudentJPA的save方法即可。123456789@RequestMapping(value = "/add") public String add()&#123; StudentEntity entity=new StudentEntity(); entity.setAddress("郑州轻工业大学"); entity.setAge(19); entity.setName("范秉洋"); studentJPA.save(entity); return "添加成功"; &#125; save方法不仅仅用于增加，如果实体类中传入主键，那么save方法就会变为根据主键更新数据库操作，下面就不再写更新的方法了。 3.1.3 删除&emsp;&emsp;直接使用JpaRepository提供的deleteById方法即可。12345@RequestMapping(value = "/delete")public String delete(Integer userId)&#123; studentJPA.deleteById(userId); return "删除成功";&#125; 3.1.4 自定义语句&emsp;&emsp;如同在查询中使用@Query注解一样，增删改查都可以使用原生SQL对数据库操作，不过@Query只能查询，那么就需要配合另一个注解@Modifying一起使用。创建一个根据姓名和年龄删除数据的方法。123@Modifying@Query(value = "delete from student_entity where s_name=? and s_age=?",nativeQuery = true)public void deleteQuery(String name,Integer age); 如果这么写的话，会抛出一个TranscationRequiredException异常，意思就是当前操作需要事务，所以要在这个方法前加@Transactional开启自动化管理。 3.1.5 自定义BaseRepository&emsp;&emsp;正常情况下一个项目肯定不可能就继承一个JpaRepository接口，再使用其他模块时还需要多个接口继承，如果每一个业务数据接口都继承几个相同的接口的话，不是不可以，但是对于系统设计和代码复用性来说不是个什么好的选择。&emsp;&emsp;创建一个叫base的包，在里面添加BaseRepository接口，并继承JpaRepository。12345678/** * @Author: cp * @Date: 2019/1/20 下午10:29 * @Version 1.0 */@NoRepositoryBeanpublic interface BaseRepository&lt;T,PK extends Serializable&gt; extends JpaRepository&lt;T,PK&gt; &#123;&#125; @NoRepositoryBean:这个注解如果配置在继承了JpaRepository接口以及其他SpringDataJpa内部的接口的子接口时，子接口不被作为一个Repository创建代理实现类。 以后再创建接口继承BaseRepository就行了，他有JpaRepository所有实现方法。 3.1.6 分页查询&emsp;&emsp;对于一般项目来说分页是少不了的，当然，SpringDataJPA也内置了分页方法。&emsp;&emsp;先创建一个BaseEntity，添加几个字段：当前页码、每页条数、排序列，排序方法。12345678910111213141516171819202122232425/** * @Author: cp * @Date: 2019/1/20 下午10:45 * @Version 1.0 */public class BaseEntity &#123; /** * 默认页码 */ protected int page=1; /** * 默认分页数量 */ protected int size=2; /** * 排序列名为id */ protected String sidx="id"; /** * 排序规则 * @return */ protected String sord="desc"; /*getter and setter*/&#125; 修改StudentEntity类继承BaseEntity,由于数据不多，所以设定一页就两条。在JPAStudentController中添加cut方法，并添加对应分页逻辑。注意在分页中页码是从0开始的。1234567@RequestMapping(value = "/page") public List&lt;StudentEntity&gt; Page(Integer page)&#123; StudentEntity entity=new StudentEntity(); entity.setSize(2); entity.setPage(page); return studentJPA.findAll(PageRequest.of(entity.getPage()-1,entity.getSize())).getContent(); &#125; 从网上找的分页方法是使用PageRequest对象，不过使用时发现这个方法的构造方法在最新版本中被废弃了，查看源码后得知取而代之的是静态的PageRequest.of()方法。接下来重启项目并访问该方法。 3.1.6 排序&emsp;&emsp;BaseEntity预设好了对应字段，所以重新编辑page方法，将Sort对象添加在PageRequest.of()中就可以实现排序。12345678910111213@RequestMapping(value = "/page") public List&lt;StudentEntity&gt; Page(Integer page)&#123; StudentEntity entity=new StudentEntity(); entity.setSize(2); entity.setPage(page); entity.setSord("desc"); //获取排序对象 Sort.Direction sort_Direction=Sort.Direction.ASC.toString().equalsIgnoreCase(entity.getSord())?Sort.Direction.ASC:Sort.Direction.DESC; //设置排序对象 Sort sort=new Sort(sort_Direction,entity.getSidx()); //执行分页 return studentJPA.findAll(PageRequest.of(entity.getPage()-1,entity.getSize(),sort)).getContent(); &#125; 我们现在将顺序按照id倒序排序，SpringDataJPA对排序方式添加了一个枚举类型，创建Sort对象时也需要枚举对象，因为我们BaseEntity配置的是字符串所以上面多了一步判断排序方式返回枚举对象。重启项目。 4.总结&emsp;&emsp;基本操作就这么多了，包括了：CURD、分页、排序、自定义SQL、定义BaseRepository、事务处理等。用起来还是挺方便的，但是这并不是全部，后面还会对一对多、多对多等复杂查询进行总结，今天就先到这里吧。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>SpringDataJPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-快速搭建]]></title>
    <url>%2F2019%2F01%2F19%2FSpringBoot-%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[SpringBoot-快速搭建1.SpringBoot简述 Spring Boot makes it easy to create stand-alone production-grade Spring based Applications that you can “just run”.We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration. &emsp;&emsp;摘自官网,大致意思是创建独立的、快速的、生产级的基于Spring的应用，对Spring和第三方库进行整合，轻松上手，只需要很少的配置就可以使用。 1.1优势&emsp;&emsp;在之前使用SSH(都9102年了我估计也没多少人用Struts了吧)与SSM时，虽然相对于传统的JSP+servlet+jdbc的开发模式简化了开发流程，但是在项目初期搭建的时候相当繁琐，大致需要： 1.配置xml，加载一坨配置文件. 2.配置Spring和SpringMVC，这两个还好，可以写到一个里面，配置Struts那是相当难受. 3.配置数据源，配置日志文件. 4.配置Mybatis的Mapper文件和Hibernate的配置文件 5.各种动态扫描注解 …… &emsp;&emsp;如果是在我学Django之前,我还是可以忍受这种东西的，但是学完Django之后，这种繁琐的配置就让人觉得很难受，我一直期望着Java中有一个和Django一样一个框架就可以干完所有活而且配置还非常简单的框架。得益于我喜欢在知(bi)乎上瞎逛，我了解了SpringBoot相关的技术链，并在禹州实训期间闲的没事干的情况下蹦入了这个坑，试了之后就一个字————爽！&emsp;&emsp;基本全程就几个配置文件，而且都不是xml，所有配置都是自动配置的，而且它内置tomcat，在使用idea的情况下，idea内置的各种工具可以实现全部框架的自动生成，再这里吹一波JetBrain。 突出优势：简单、快速、方便的搭建项目主流开发框架无配置集成提高开发、部署效率 2.项目搭建2.1开发环境 os:MacOS 10.13.4 ide:IDEA 2018.1.4 jdk:10.0.1 maven:3.3.9 tomcat:9.0(框架自带) 2.2新建项目&emsp;&emsp;新建项目时，点击左侧的Spring Initializr,然后点击next。 &emsp;&emsp;下一个页面是修改项目信息，第一个参数Group的域，第二个参数Artifact一般代表公司名称，这两个参数都是提供给maven的。最后一个参数Package为项目总包名，余下几个参数，Type一般都选择maven，Languge一般选择Java，也可以使用其他虚拟机语言，比如Kotlin、Groovy。Packing是打包方式，可以选择打成jar包或者war包 钩上web模版选择位置&emsp;&emsp;因为依托与maven，第一次加载时间可能比较长，因为所有配置需要从网络上去拉取，下载完毕后文件目录如图所示。看一下目录，大致分为三块： java文件：代码源文件，逻辑代码都在这里，里面那个ZzuliApplicaion是系统入口文件。 resource文件：静态资源文件，里面放着所有的静态资源和配置文件，application.properties中可以添加其他配置。 test文件：测试模块，里面内涵一个Junit测试。 外面有一个pom.xml文件，这个是maven的配置文件。 2.3 测试运行&emsp;&emsp;在之前创建的cn.edu.zzuli文件夹下创建Controller文件夹，并创建HelloWorldController类进行测试。1234567891011121314151617package cn.edu.zzuli.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @Author: cp * @Date: 2019/1/19 下午4:38 * @Version 1.0 */@RestControllerpublic class HelloWorldController &#123; @RequestMapping("/php") public String php()&#123; return "php是世界上最好的语言"; &#125;&#125; 基本上还是和SpringMVC很像的，唯一一点不同就是@RestController注解，这个注解就是原来@Controller和@ResponseBody注解的合体版。&emsp;&emsp;直接点击右上角的箭头或者启动ZzuliApplicaion类。出现这个界面就说明SpringBoot启动成功了。接着在浏览器中直接访问http://localhost:8080/php信息已被返回。 3.分析项目3.1 pox.xml文件&emsp;&emsp;SpringBoot自动生成的pox.xml文件如下12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.edu&lt;/groupId&gt; &lt;artifactId&gt;zzuli&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;zzuli&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 相较于一般的pox.xml文件，这里多了一个&lt;parent&gt;标签,这个标签是在配置SpringBoot的父级依赖,有了这个，当前的项目才是SpringBoot项目，spring-boot-starter-parent是一个特殊的starter，它用来提供相关的maven默认依赖，使用它之后，常用的包依赖就可以省去version标签。&emsp;&emsp;\repository\org\springframework\boot\spring-boot-dependencies\2.0.1.RELEASE\spring-boot-dependencies-2.0.1.RELEASE.pom里面有SpringBoot的所有依赖，想了解的去这里看看 3.2 入口类&emsp;&emsp;SpringBoot项目通常有一个名为*Application的入口类，入口类里有一个main方法，这个main方法其实就是一个标准的Java应用的入口方法。&emsp;&emsp;@SpringBootApplication是SpringBoot的核心注解，它是一个组合注解，该注解组合了：@Configuration、@EnableAutoConfiguration、@ComponentScan若不是用@SpringBootApplication注解也可以使用这三个注解代替。&emsp;&emsp;其中，EnableAutoConfiguration让SpringBoot根据类路径中的jar包依赖为当前项目进行自动配置，例如，添加了spring-boot-starter-web依赖，会自动添加Tomcat和SpringMVC的依赖，那么SpringBoot会对Tomcat和SpringMVC进行自动配置。&emsp;&emsp;SpringBoot还会自动扫描@SpringBootApplication所在类的同级包以及下级包里的Bean，所以入口类建议就配置在grounpID+arctifactID组合的包名下（这里为cn.edu.zzuli包），使用idea自动创建项目的话idea会自动在这里创建。 3.3 配置文件&emsp;&emsp;SpringBoot使用一个全局的配置文件application.properties或application.yml，放置在src/main/resources目录或者类路径的/config下。&emsp;&emsp;SpringBoot不仅支持常规的properties配置文件，还支持yaml语言的配置文件。yaml是以数据为中心的语言，在配置数据的时候具有面向对象的特征。&emsp;&emsp;SpringBoot的全局配置文件的作用是对一些默认配置的配置值进行修改。这里我将application.properties修改为application.yml。 4.使用SpringBoot&emsp;&emsp;上面只是一些简单的搭建以及运行，下面再深入了解一下SpringBoot的应用。SpringBoot的默认视图支持是 Thymeleaf 模板引擎。但是咱不会，只好还用JSP。 4.1集成JSP1.在pom.xml文件中集成JSP，向pom.xml中添加1234567891011121314&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 2.在application.yml文件配置视图解析器，将我们的 JSP 文件重定向到 /WEB-INF/views/ 目录下：12345spring: mvc: view: prefix: /WEB-INF/views/ suffix: .jsp 3.修改@RestController注解为@Controller(不改写不能写jsp)，然后修改 php 方法： 12345678910111213/** * @Author: cp * @Date: 2019/1/19 下午4:38 * @Version 1.0 */@Controllerpublic class HelloWorldController &#123; @RequestMapping("/php") public String php(Model model)&#123; model.addAttribute("php","php是世界上最好的语言"); return "php"; &#125;&#125; 4.在src/main目录下依次创建webapp/WEB-INF/views目录，并创建一个 hello.jsp 文件： 123&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8" %&gt;谁是世界上最好的语言？&lt;br&gt;$&#123;php&#125; 再刷新页面就可以看到效果了。 4.2集成MyBatis1.在pox.xml中添加对mysql和mybatis对依赖123456789101112&lt;!-- mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.21&lt;/version&gt;&lt;/dependency&gt; 2.在配置文件中添加配置12345678910111213spring: mvc: view: prefix: /WEB-INF/views/ suffix: .jsp datasource: url: jdbc:mysql://127.0.0.1:3306/test3?characterEncoding=UTF-8 username: root password: 20141232 driver-class-name: com.mysql.jdbc.Driver jpa: hibernate: ddl-auto: update 3.先在cn.edu.zzuli中创建domain包和mapper包，再创建Student实体类和StudentMapper映射类分别放入domain和mapper包,1234567891011121314151617181920212223242526272829package cn.edu.zzuli.domain;/** * @Author: cp * @Date: 2019/1/19 下午6:35 * @Version 1.0 */public class Student &#123; int id; String name; int age; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 123456789101112131415161718package cn.edu.zzuli.mapper;import cn.edu.zzuli.domain.Student;import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Select;import java.util.List;/** * @Author: cp * @Date: 2019/1/19 下午6:41 * @Version 1.0 */@Mapperpublic interface StudentMapper &#123; @Select("select * from student") List&lt;Student&gt; findAll();&#125; 4.写一个StudentContorller 123456789101112131415161718192021222324252627package cn.edu.zzuli.Controller;import cn.edu.zzuli.domain.Student;import cn.edu.zzuli.mapper.StudentMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import java.util.List;/** * @Author: cp * @Date: 2019/1/19 下午6:44 * @Version 1.0 */@Controllerpublic class StudentController &#123; @Autowired StudentMapper studentMapper; @RequestMapping("/studentlist") public String studentlist(Model model)&#123; List&lt;Student&gt; students=studentMapper.findAll(); model.addAttribute("list",students); return "list"; &#125;&#125; 5.写一个list.jsp文件 1234567891011121314151617&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c"%&gt;&lt;table align='center' border='1' cellspacing='0'&gt; &lt;tr&gt; &lt;td&gt;id&lt;/td&gt; &lt;td&gt;name&lt;/td&gt; &lt;/tr&gt; &lt;c:forEach items="$&#123;list&#125;" var="s" varStatus="st"&gt; &lt;tr&gt; &lt;td&gt;$&#123;s.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;s.name&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt;&lt;/table&gt; 刷新网络，大功告成。 以上就是springboot的搭建，随后的时间里会对Springboot的其他模块进行更深入的探索。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F17%2Fhello-world%2F</url>
    <content type="text"><![CDATA[我的第一篇博客]]></content>
  </entry>
</search>
